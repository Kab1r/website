---
title: Service
feature:
  title: D√©couverte de services et √©quilibrage de charge
  description: >
    Pas besoin de modifier votre application pour utiliser un m√©canisme de
    d√©couverte de services inconnu. Kubernetes donne aux pods leurs propres
    adresses IP et un nom DNS unique pour un ensemble de pods, et peut
    √©quilibrer la charge entre eux.

content_template: templates/concept
weight: 10
---

{{% capture overview %}}

{{< glossary_definition term_id="service" length="short" >}}

Avec Kubernetes, vous n'avez pas besoin de modifier votre application pour
utiliser un m√©canisme de d√©couverte de services inconnu. Kubernetes donne aux
pods leurs propres adresses IP et un nom DNS unique pour un ensemble de pods, et
peut √©quilibrer la charge entre eux.

{{% /capture %}}

{{% capture body %}}

## Motivation

Les {{< glossary_tooltip term_id="pod" text="Pods" >}} Kubernetes sont mortels.
Ils naissent et lorsqu'ils meurent, ils ne ressuscitent pas. Si vous utilisez un
{{< glossary_tooltip term_id="deployment" >}} pour ex√©cuter votre application,
il peut cr√©er et d√©truire dynamiquement des pods.

Chaque pod obtient sa propre adresse IP, mais dans un d√©ploiement, l'ensemble de
pods s'ex√©cutant en un instant peut √™tre diff√©rent de l'ensemble de pods
ex√©cutant cette application un instant plus tard.

Cela conduit √† un probl√®me: si un ensemble de pods (appelez-les ¬´backends¬ª)
fournit des fonctionnalit√©s √† d'autres pods (appelez-les ¬´frontends¬ª) √
l'int√©rieur de votre cluster, comment les frontends peuvent-ils trouver et
suivre l'adresse IP √† laquelle se connecter, afin que le frontend puisse
utiliser la partie backend de la charge de travail?

C'est l√† o√π les _Services_ rentrent en jeu.

## La ressource Service {#service-resource}

Dans Kubernetes, un service est une abstraction qui d√©finit un ensemble logique
de pods et une politique permettant d'y acc√©der (parfois ce mod√®le est appel√© un
micro-service). L'ensemble des pods cibl√©s par un service est g√©n√©ralement
d√©termin√© par un {{< glossary_tooltip text="selector" term_id="selector" >}}
(voir [ci-dessous](#services-without-selectors) pourquoi vous voudrez peut-√™tre
un service _sans_ un s√©lecteur).

Par exemple, consid√©rons un backend de traitement d'image sans √©tat qui
s'ex√©cute avec 3 replicas. Ces r√©plicas sont fongibles et les frontends ne se
soucient pas du backend qu'ils utilisent. Bien que les pods r√©els qui composent
l'ensemble backend puissent changer, les clients frontends ne devraient pas
avoir besoin de le savoir, pas plus qu'ils ne doivent suivre eux-m√™mes
l'ensemble des backends.

L'abstraction du service permet ce d√©couplage.

### D√©couverte de services native du cloud

Si vous pouvez utiliser les API Kubernetes pour la d√©couverte de services dans
votre application, vous pouvez interroger
l'{{< glossary_tooltip text="API server" term_id="kube-apiserver" >}} pour les
Endpoints, qui sont mis √† jour chaque fois que l'ensemble des pods d'un service
change.

Pour les applications non natives, Kubernetes propose des moyens de placer un
port r√©seau ou un load balancer entre votre application et les modules backend.

## D√©finition d'un service

Un service dans Kubernetes est un objet REST, semblable √† un pod. Comme tous les
objets REST, vous pouvez effectuer un `POST` d'une d√©finition de service sur le
serveur API pour cr√©er une nouvelle instance.

Par exemple, supposons que vous ayez un ensemble de pods qui √©coutent chacun sur
le port TCP 9376 et portent une √©tiquette `app=MyApp`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

Cette sp√©cification cr√©e un nouvel objet Service nomm√© ¬´my-service¬ª, qui cible
le port TCP 9376 sur n'importe quel pod avec l'√©tiquette ¬´app=MyApp¬ª.

Kubernetes attribue √† ce service une adresse IP (parfois appel√© l'"IP cluster"),
qui est utilis√© par les proxies Service (voir
[IP virtuelles et proxy de service](#virtual-ips-and-service-proxies)).

Le contr√¥leur de service recherche en continu les pods qui correspondent √† son
s√©lecteur, puis POST toutes les mises √† jour d'un objet Endpoint √©galement
appel√© "my-service".

{{< note >}} Un service peut mapper _n'importe quel_ `port` entrant vers un
`targetPort`. Par d√©faut et pour plus de commodit√©, le `targetPort` a la m√™me
valeur que le champ `port`. {{< /note >}}

Les d√©finitions de port dans les pods ont des noms, et vous pouvez r√©f√©rencer
ces noms dans l'attribut `targetPort` d'un service. Cela fonctionne m√™me s'il
existe un m√©lange de pods dans le service utilisant un seul nom configur√©, avec
le m√™me protocole r√©seau disponible via diff√©rents num√©ros de port. Cela offre
beaucoup de flexibilit√© pour d√©ployer et faire √©voluer vos services. Par
exemple, vous pouvez modifier les num√©ros de port que les pods exposent dans la
prochaine version de votre logiciel principal, sans casser les clients.

Le protocole par d√©faut pour les services est TCP; vous pouvez √©galement
utiliser tout autre [protocole pris en charge](#protocol-support).

Comme de nombreux services doivent exposer plus d'un port, Kubernetes prend en
charge plusieurs d√©finitions de port sur un objet Service. Chaque d√©finition de
port peut avoir le m√™me protocole, ou un autre.

### Services sans s√©lecteurs

Les services abritent le plus souvent l'acc√®s aux pods Kubernetes, mais ils
peuvent √©galement abstraire d'autres types de backends. Par exemple:

- Vous voulez avoir un cluster de base de donn√©es externe en production, mais
  dans votre environnement de test, vous utilisez vos propres bases de donn√©es.
- Vous souhaitez pointer votre service vers un service dans un autre
  {{< glossary_tooltip term_id="namespace" >}} ou sur un autre cluster.
- Vous migrez une charge de travail vers Kubernetes. Lors de l'√©valuation de
  l'approche, vous ex√©cutez uniquement une partie de vos backends dans
  Kubernetes.

Dans n'importe lequel de ces sc√©narios, vous pouvez d√©finir un service _sans_ un
s√©lecteur de pod. Par exemple:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

√âtant donn√© que ce service n'a pas de s√©lecteur, l'objet Endpoint correspondant
n'est _pas_ cr√©√© automatiquement. Vous pouvez mapper manuellement le service √
l'adresse r√©seau et au port o√π il s'ex√©cute, en ajoutant manuellement un objet
Endpoint:

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: 192.0.2.42
    ports:
      - port: 9376
```

{{< note >}} Les IP de noeud final ne doivent pas √™tre: loopback (127.0.0.0/8
pour IPv4, ::1/128 pour IPv6), ou link-local (169.254.0.0/16 et 224.0.0.0/24
pour IPv4, fe80::/64 pour IPv6).

Les adresses IP de noeud final ne peuvent pas √™tre les adresses IP de cluster
d'autres services Kubernetes, car {{< glossary_tooltip term_id="kube-proxy" >}}
ne prend pas en charge les adresses IP virtuelles en tant que destination.
{{< /note >}}

L'acc√®s √† un service sans s√©lecteur fonctionne de la m√™me mani√®re que s'il avait
un s√©lecteur. Dans l'exemple ci-dessus, le trafic est rout√© vers le Endpoint
unique d√©fini dans le YAML: `192.0.2.42:9376` (TCP).

Un service ExternalName est un cas sp√©cial de service qui n'a pas de s√©lecteurs
et utilise des noms DNS √† la place. Pour plus d'informations, consultez la
section [ExternalName](#externalname) plus loin dans ce document.

### Endpoint Slices

{{< feature-state for_k8s_version="v1.17" state="beta" >}}

Un Endpoint Slices est une ressource API qui peut fournir une alternative plus
√©volutive au Endpoints. Bien que conceptuellement assez similaire aux Endpoints,
les Endpoint Slices permettent la distribution des endpoints r√©seau sur
plusieurs ressources. Par d√©faut, un Endpoint Slice est consid√©r√© comme "plein"
une fois qu'il atteint 100 endpoints, au del√†, des Endpoint Slices addtionnels
seront cr√©es pour stocker tout autre endpoints.

Les Endpoint Slices fournissent des attributs et des fonctionnalit√©s
suppl√©mentaires qui sont d√©crits en d√©tail dans
[Endpoint Slices](/docs/concepts/services-networking/endpoint-slices/).

## IP virtuelles et proxy de service

Chaque n≈ìud d'un cluster Kubernetes ex√©cute un `kube-proxy`. `kube-proxy` est
responsable de l'impl√©mentation d'une forme d'IP virtuelle pour les `Services`
qui ne sont pas de type [`ExternalName`](#externalname).

### Pourquoi ne pas utiliser le DNS round-robin ?

Une question qui appara√Æt de temps en temps est pourquoi Kubernetes s'appuie sur
le proxy pour transf√©rer le trafic entrant vers les backends. Et les autres
approches? Par exemple, serait-il possible de configurer des enregistrements DNS
qui ont plusieurs valeurs A (ou AAAA pour IPv6), et de s'appuyer sur la
r√©solution de nom √† tour de r√¥le (round-robin)?

Il existe plusieurs raisons d'utiliser le proxy pour les services:

- Il existe une longue histoire d'impl√©mentations DNS ne respectant pas les TTL
  d'enregistrement et mettant en cache les r√©sultats des recherches de noms
  apr√®s leur expiration.
- Certaines applications n'effectuent des recherches DNS qu'une seule fois et
  mettent en cache les r√©sultats ind√©finiment.
- M√™me si les applications et les biblioth√®ques ont fait une bonne r√©solution,
  les TTL faibles ou nuls sur les enregistrements DNS pourraient imposer une
  charge √©lev√©e sur DNS qui devient alors difficile √† g√©rer.

### User space proxy mode {#proxy-mode-userspace}

Dans ce mode, kube-proxy surveille le ma√Ætre Kubernetes pour l'ajout et la
suppression d'objets Service et Endpoint. Pour chaque service, il ouvre un port
(choisi au hasard) sur le n≈ìud local. Toutes les connexions √† ce "port proxy"
sont transmises par proxy √† l'un des modules backend du service (comme indiqu√©
via les Endpoints). kube-proxy prend en compte le param√®tre `SessionAffinity` du
service pour d√©cider quel pod backend utiliser.

Enfin, le proxy de l'espace utilisateur installe des r√®gles iptables qui
capturent le trafic vers le service `clusterIP` (qui est virtuel) et `port`. Les
r√®gles redirigent ce trafic vers le port proxy qui fait office de proxy pour le
Pod de backend.

Par d√©faut, kube-proxy en mode espace utilisateur choisit un backend via un
algorithme round-robin.

![Diagramme de vue d'ensemble des services pour le proxy de l'espace utilisateur](/images/docs/services-userspace-overview.svg)

### `iptables` proxy mode {#proxy-mode-iptables}

Dans ce mode, kube-proxy surveille le plan de contr√¥le Kubernetes pour l'ajout
et la suppression d'objets Service et Endpoint. Pour chaque service, il installe
des r√®gles iptables, qui capturent le trafic vers le ¬´clusterIP¬ª et le ¬´port¬ª du
service, et redirigent ce trafic vers l'un des ensembles principaux du service.
Pour chaque objet Endpoint, il installe des r√®gles iptables qui s√©lectionnent un
Pod de backend.

Par d√©faut, kube-proxy en mode iptables choisit un backend au hasard.

L'utilisation d'iptables pour g√©rer le trafic a un co√ªt syst√®me inf√©rieur, car
le trafic est g√©r√© par Linux netfilter sans avoir besoin de basculer entre
l'espace utilisateur et l'espace noyau. Cette approche est √©galement susceptible
d'√™tre plus fiable.

Si kube-proxy s'ex√©cute en mode iptables et que le premier pod s√©lectionn√© ne
r√©pond pas, la connexion √©choue. C'est diff√©rent du mode espace utilisateur:
dans ce sc√©nario, kube-proxy d√©tecterait que la connexion au premier pod avait
√©chou√© et r√©essayerait automatiquement avec un pod backend diff√©rent.

Vous pouvez utiliser les
[readiness probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)
d'un Pod pour v√©rifier que les pods backend fonctionnent correctement, de sorte
que kube-proxy en mode iptables ne voit que les backends test√©s comme sains.
Cela signifie que vous √©vitez d'envoyer du trafic via kube-proxy vers un pod
connu pour avoir √©chou√©.

![Diagramme de pr√©sentation des services pour le proxy iptables](/images/docs/services-iptables-overview.svg)

### IPVS proxy mode {#proxy-mode-ipvs}

{{< feature-state for_k8s_version="v1.11" state="stable" >}}

En mode `ipvs`, kube-proxy surveille les Services et Endpoints Kubernetes.
kube-proxy appelle l'interface`netlink` pour cr√©er les r√®gles IPVS en
cons√©quence et synchronise p√©riodiquement les r√®gles IPVS avec les Services et
Endpoints Kubernetes. Cette boucle de contr√¥le garantit que l'√©tat IPVS
correspond √† l'√©tat souhait√©. Lors de l'acc√®s √† un service, IPVS dirige le
trafic vers l'un des pods backend.

Le mode proxy IPVS est bas√© sur des fonctions hooks de netfilter qui est
similaire au mode iptables, mais utilise la table de hachage comme structure de
donn√©es sous-jacente et fonctionne dans l'espace du noyau. Cela signifie que
kube-proxy en mode IPVS redirige le trafic avec une latence plus faible que
kube-proxy en mode iptables, avec de bien meilleures performances lors de la
synchronisation des r√®gles de proxy. Par rapport aux autres modes proxy, le mode
IPVS prend √©galement en charge un d√©bit plus √©lev√© de trafic r√©seau.

IPVS offre plus d'options pour √©quilibrer le trafic vers les pods
d'arri√®re-plan; ceux-ci sont:

- `rr`: round-robin
- `lc`: least connection (plus petit nombre de connexions ouvertes)
- `dh`: destination hashing
- `sh`: source hashing
- `sed`: shortest expected delay
- `nq`: never queue

{{< note >}} Pour ex√©cuter kube-proxy en mode IPVS, vous devez rendre IPVS Linux
disponible sur le n≈ìud avant de d√©marrer kube-proxy.

Lorsque kube-proxy d√©marre en mode proxy IPVS, il v√©rifie si les modules du
noyau IPVS sont disponibles. Si les modules du noyau IPVS ne sont pas d√©tect√©s,
alors kube-proxy revient √† fonctionner en mode proxy iptables. {{< /note >}}

![Diagramme de vue d'ensemble des services pour le proxy IPVS](/images/docs/services-ipvs-overview.svg)

Dans ces mod√®les de proxy, le trafic li√© √† l'IP: Port du service est dirig√© vers
un backend appropri√© sans que les clients ne sachent quoi que ce soit sur
Kubernetes, les services ou les pods.

Si vous souhaitez vous assurer que les connexions d'un client particulier sont
transmises √† chaque fois au m√™me pod, vous pouvez s√©lectionner l'affinit√© de
session en fonction des adresses IP du client en d√©finissant
`service.spec.sessionAffinity` sur" ClientIP "(la valeur par d√©faut est" None").
Vous pouvez √©galement d√©finir la dur√©e maximale de session persistante en
d√©finissant `service.spec.sessionAffinityConfig.clientIP.timeoutSeconds` de
mani√®re appropri√©e (la valeur par d√©faut est 10800, ce qui correspond √† 3
heures).

## Services multi-ports

Pour certains services, vous devez exposer plusieurs ports. Kubernetes vous
permet de configurer plusieurs d√©finitions de port sur un objet Service. Lorsque
vous utilisez plusieurs ports pour un service, vous devez donner tous vos noms
de ports afin qu'ils ne soient pas ambigus. Par exemple:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377
```

{{< note >}} Comme pour tous les
{{< glossary_tooltip term_id="name" text="names">}} Kubernetes en g√©n√©ral, les
noms de ports ne doivent contenir que des caract√®res alphanum√©riques en
minuscules et `-`. Les noms de port doivent √©galement commencer et se terminer
par un caract√®re alphanum√©rique.

Par exemple, les noms `123-abc` et `web` sont valides, mais `123_abc` et `-web`
ne le sont pas. {{< /note >}}

## Choisir sa propre adresse IP

Vous pouvez sp√©cifier votre propre adresse IP de cluster dans le cadre d'une
demande de cr√©ation de Service. Pour ce faire, d√©finissez le champ
`.spec.clusterIP`. Par exemple, si vous avez d√©j√† une entr√©e DNS existante que
vous souhaitez r√©utiliser, ou des syst√®mes existants qui sont configur√©s pour
une adresse IP sp√©cifique et difficiles √† reconfigurer.

L'adresse IP que vous choisissez doit √™tre une adresse IPv4 ou IPv6 valide dans
la plage CIDR `service-cluster-ip-range` configur√©e pour le serveur API. Si vous
essayez de cr√©er un service avec une valeur d'adresse de clusterIP non valide,
le serveur API retournera un code d'√©tat HTTP 422 pour indiquer qu'il y a un
probl√®me.

## D√©couvrir les services

Kubernetes prend en charge 2 modes principaux de recherche d'un service: les
variables d'environnement et DNS.

### Variables d'environnement

Lorsqu'un pod est ex√©cut√© sur un n≈ìud, le kubelet ajoute un ensemble de
variables d'environnement pour chaque service actif. Il prend en charge √† la
fois les variables
[Docker links](https://docs.docker.com/userguide/dockerlinks/) (voir
[makeLinkVariables](http://releases.k8s.io/{{< param
"githubbranch" >}}/pkg/kubelet/envvars/envvars.go#L49)) et plus simplement les
variables `{SVCNAME}_SERVICE_HOST` et `{SVCNAME}_SERVICE_PORT`, o√π le nom du
service est en majuscules et les tirets sont convertis en underscore.

Par exemple, le service `redis-master` qui expose le port TCP 6379 et a re√ßu
l'adresse IP de cluster 10.0.0.11, produit les variables d'environnement
suivantes:

```shell
REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11
```

{{< note >}} Lorsque vous avez un pod qui doit acc√©der √† un service et que vous
utilisez la m√©thode des variables d'environnement pour publier le port et l'IP
du cluster sur les pods clients, vous devez cr√©er le service _avant_ que les
pods clients n'existent. Sinon, ces pods clients n'auront pas leurs variables
d'environnement remplies.

Si vous utilisez uniquement DNS pour d√©couvrir l'IP du cluster pour un service,
vous n'avez pas √† vous soucier de ce probl√®me de commande. {{< /note >}}

### DNS

Vous pouvez (et devriez presque toujours) configurer un service DNS pour votre
cluster Kubernetes √† l'aide d'un
[add-on](/docs/concepts/cluster-administration/addons/).

Un serveur DNS prenant en charge les clusters, tel que CoreDNS, surveille l'API
Kubernetes pour les nouveaux services et cr√©e un ensemble d'enregistrements DNS
pour chacun. Si le DNS a √©t√© activ√© dans votre cluster, tous les pods devraient
automatiquement √™tre en mesure de r√©soudre les services par leur nom DNS.

Par exemple, si vous avez un service appel√© `"my-service"` dans un namespace
Kubernetes `"my-ns"`, le plan de contr√¥le et le service DNS agissant ensemble et
cr√©ent un enregistrement DNS pour `"my-service.my-ns"`. Les Pods dans le
Namespace `"my-ns"` devrait √™tre en mesure de le trouver en faisant simplement
une recherche de nom pour `my-service` (`"my-service.my-ns"` fonctionnerait
√©galement).

Les pods dans d'autres namespaces doivent utiliser le nom de `my-service.my-ns`.
Ces noms seront r√©solus en IP de cluster attribu√©e pour le service.

Kubernetes prend √©galement en charge les enregistrements DNS SRV (Service) pour
les ports nomm√©s. Si le service `"my-service.my-ns"` a un port nomm√© `http` avec
un protocole d√©fini sur `TCP`, vous pouvez effectuer une requ√™te DNS SRV pour
`_http._tcp.my-service.my-ns` pour d√©couvrir le num√©ro de port de `http`, ainsi
que l'adresse IP.

Le serveur DNS Kubernetes est le seul moyen d'acc√©der aux services
`ExternalName`. Vous pouvez trouver plus d'informations sur la r√©solution de
`ExternalName` dans
[DNS Pods et Services](/docs/concepts/services-networking/dns-pod-service/).

## Headless Services

Parfois, vous n'avez pas besoin de load-balancing et d'une seule IP de Service.
Dans ce cas, vous pouvez cr√©er ce que l'on appelle des services "headless", en
sp√©cifiant explicitement "None" pour l'IP du cluster (`.spec.clusterIP`).

Vous pouvez utiliser un service headless pour interfacer avec d'autres
m√©canismes de d√©couverte de service, sans √™tre li√© √† l'impl√©mentation de
Kubernetes.

Pour les services headless, une IP de cluster n'est pas allou√©e, kube-proxy ne
g√®re pas ces services et aucun load-balancing ou proxy n'est effectu√© par la
plateforme pour eux. La configuration automatique de DNS d√©pend de la d√©finition
ou non de s√©lecteurs par le service:

### Avec s√©lecteurs

Pour les services headless qui d√©finissent des s√©lecteurs, le controlleur des
Endpoints cr√©e des enregistrements `Endpoints` dans l'API, et modifie la
configuration DNS pour renvoyer des enregistrements (adresses) qui pointent
directement vers les `Pods` vis√©s par le `Service`.

### Sans s√©lecteurs

Pour les services headless qui ne d√©finissent pas de s√©lecteurs, le contr√¥leur
des Endpoints ne cr√©e pas d'enregistrements `Endpoints`. Cependant, le syst√®me
DNS recherche et configure soit:

- Enregistrements CNAME pour les services de type
  [`ExternalName`](#externalname).
- Un enregistrement pour tous les ¬´Endpoints¬ª qui partagent un nom avec le
  Service, pour tous les autres types.

## Services de publication (ServiceTypes) {#publishing-services-service-types}

Pour certaines parties de votre application (par exemple, les frontaux), vous
souhaiterez peut-√™tre exposer un service sur une adresse IP externe, qui est en
dehors de votre cluster.

Les ¬´ServiceTypes¬ª de Kubernetes vous permettent de sp√©cifier le type de service
que vous souhaitez. La valeur par d√©faut est ¬´ClusterIP¬ª.

Les valeurs de `Type` et leurs comportements sont:

- `ClusterIP`: Expose le service sur une IP interne au cluster. Le choix de
  cette valeur rend le service uniquement accessible √† partir du cluster. Il
  s'agit du `ServiceType` par d√©faut.
- [`NodePort`](#nodeport): Expose le service sur l'IP de chaque n≈ìud sur un port
  statique (le `NodePort`). Un service `ClusterIP`, vers lequel le
  service`NodePort` est automatiquement cr√©√©. Vous pourrez contacter le service
  `NodePort`, depuis l'ext√©rieur du cluster, en demandant
  `<NodeIP>: <NodePort>`.
- [`LoadBalancer`](#loadbalancer): Expose le service en externe √† l'aide de
  l'√©quilibreur de charge d'un fournisseur de cloud. Les services `NodePort` et
  `ClusterIP`, vers lesquels les itin√©raires de l'√©quilibreur de charge externe,
  sont automatiquement cr√©√©s.
- [`ExternalName`](#externalname): Mappe le service au contenu du champ
  `externalName` (par exemple`foo.bar.example.com`), en renvoyant un
  enregistrement `CNAME` avec sa valeur. Aucun proxy d'aucune sorte n'est mis en
  place. {{< note >}} Vous avez besoin de CoreDNS version 1.7 ou sup√©rieure pour
  utiliser le type `ExternalName`. {{< /note >}}

Vous pouvez √©galement utiliser
[Ingress](/fr/docs/concepts/services-networking/ingress) pour exposer votre
service. Ingress n'est pas un type de service, mais il sert de point d'entr√©e
pour votre cluster. Il vous permet de consolider vos r√®gles de routage en une
seule ressource car il peut exposer plusieurs services sous la m√™me adresse IP.

### Type NodePort {#nodeport}

Si vous d√©finissez le champ `type` sur`NodePort`, le plan de contr√¥le Kubernetes
alloue un port √† partir d'une plage sp√©cifi√©e par l'indicateur
`--service-node-port-range` (par d√©faut: 30000-32767). Chaque n≈ìud assure le
proxy de ce port (le m√™me num√©ro de port sur chaque n≈ìud) vers votre service.
Votre service signale le port allou√© dans son champ `.spec.ports[*].nodePort`.

Si vous souhaitez sp√©cifier une ou des adresses IP particuli√®res pour proxyfier
le port, vous pouvez d√©finir l'indicateur `--nodeport-addresses` dans kube-proxy
sur des blocs IP particuliers; cela est pris en charge depuis Kubernetes v1.10.
Cet indicateur prend une liste d√©limit√©e par des virgules de blocs IP (par
exemple 10.0.0.0/8, 192.0.2.0/25) pour sp√©cifier les plages d'adresses IP que
kube-proxy doit consid√©rer comme locales pour ce n≈ìud.

Par exemple, si vous d√©marrez kube-proxy avec l'indicateur
`--nodeport-addresses=127.0.0.0/8`, kube-proxy s√©lectionne uniquement
l'interface de boucle locale pour les services NodePort. La valeur par d√©faut
pour `--nodeport-addresses` est une liste vide. Cela signifie que kube-proxy
doit prendre en compte toutes les interfaces r√©seau disponibles pour NodePort
(qui est √©galement compatible avec les versions ant√©rieures de Kubernetes).

Si vous voulez un num√©ro de port sp√©cifique, vous pouvez sp√©cifier une valeur
dans le champ `nodePort`. Le plan de contr√¥le vous attribuera ce port ou
signalera l'√©chec de la transaction API. Cela signifie que vous devez vous
occuper vous-m√™me des √©ventuelles collisions de ports. Vous devez √©galement
utiliser un num√©ro de port valide, celui qui se trouve dans la plage configur√©e
pour l'utilisation de NodePort.

L'utilisation d'un NodePort vous donne la libert√© de configurer votre propre
solution d'√©quilibrage de charge, de configurer des environnements qui ne sont
pas enti√®rement pris en charge par Kubernetes, ou m√™me d'exposer directement les
adresses IP d'un ou plusieurs n≈ìuds.

Notez que ce service est visible en tant que `<NodeIP>: spec.ports[*].nodePort`
et `.spec.clusterIP: spec.ports[*].Port`. (Si l'indicateur
`--nodeport-addresses` dans kube-proxy est d√©fini, <NodeIP> serait filtr√©
NodeIP(s).)

### Type LoadBalancer {#loadbalancer}

Sur les fournisseurs de cloud qui prennent en charge les load balancers
externes, la d√©finition du champ `type` sur`LoadBalancer` provisionne un load
balancer pour votre service. La cr√©ation r√©elle du load balancer se produit de
mani√®re asynchrone et les informations sur le load balancer provisionn√© sont
publi√©es dans le champ `.status.loadBalancer`. Par exemple:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
      - ip: 192.0.2.127
```

Le trafic provenant du load balancer externe est dirig√© vers les Pods backend.
Le fournisseur de cloud d√©cide de la r√©partition de la charge.

Certains fournisseurs de cloud vous permettent de sp√©cifier le `loadBalancerIP`.
Dans ces cas, le load balancer est cr√©√© avec le `loadBalancerIP` sp√©cifi√© par
l'utilisateur. Si le champ `loadBalancerIP` n'est pas sp√©cifi√©, le loadBalancer
est configur√© avec une adresse IP √©ph√©m√®re. Si vous sp√©cifiez un
`loadBalancerIP` mais que votre fournisseur de cloud ne prend pas en charge la
fonctionnalit√©, le champ `loadBalancerIP` que vous d√©finissez est ignor√©.

{{< note >}} Si vous utilisez SCTP, voir le
[caveat](#caveat-sctp-loadbalancer-service-type) ci-dessous sur le type de
service `LoadBalancer`. {{< /note >}}

{{< note >}}

Sur **Azure**, si vous souhaitez utiliser un type public sp√©cifi√© par
l'utilisateur `loadBalancerIP`, vous devez d'abord cr√©er une ressource d'adresse
IP publique de type statique. Cette ressource d'adresse IP publique doit se
trouver dans le m√™me groupe de ressources que les autres ressources cr√©√©es
automatiquement du cluster. Par exemple,
`MC_myResourceGroup_myAKSCluster_eastus`.

Sp√©cifiez l'adresse IP attribu√©e en tant que loadBalancerIP. Assurez-vous
d'avoir mis √† jour le securityGroupName dans le fichier de configuration du
fournisseur de cloud. Pour plus d'informations sur le d√©pannage
`CreatingLoadBalancerFailed` relatif aux permissions consultez:
[Use a static IP address with the Azure Kubernetes Service (AKS) load balancer](https://docs.microsoft.com/en-us/azure/aks/static-ip)
ou
[CreatingLoadBalancerFailed on AKS cluster with advanced networking](https://github.com/Azure/AKS/issues/357).

{{< /note >}}

#### Load Balancer interne

Dans un environnement mixte, il est parfois n√©cessaire d'acheminer le trafic des
services √† l'int√©rieur du m√™me bloc d'adresse r√©seau (virtuel).

Dans un environnement DNS √† horizon divis√©, vous auriez besoin de deux services
pour pouvoir acheminer le trafic externe et interne vers vos endpoints.

Vous pouvez y parvenir en ajoutant une des annotations suivantes √† un service.
L'annotation √† ajouter d√©pend du fournisseur de services cloud que vous
utilisez.

{{< tabs name="service_tabs" >}} {{% tab name="Default" %}} S√©lectionnez l'un
des onglets. {{% /tab %}} {{% tab name="GCP" %}}

```yaml
[...]
metadata:
    name: my-service
    annotations:
        cloud.google.com/load-balancer-type: "Internal"
[...]
```

{{% /tab %}} {{% tab name="AWS" %}}

```yaml
[...]
metadata:
    name: my-service
    annotations:
        service.beta.kubernetes.io/aws-load-balancer-internal: "true"
[...]
```

{{% /tab %}} {{% tab name="Azure" %}}

```yaml
[...]
metadata:
    name: my-service
    annotations:
        service.beta.kubernetes.io/azure-load-balancer-internal: "true"
[...]
```

{{% /tab %}} {{% tab name="OpenStack" %}}

```yaml
[...]
metadata:
    name: my-service
    annotations:
        service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
[...]
```

{{% /tab %}} {{% tab name="Baidu Cloud" %}}

```yaml
[...]
metadata:
    name: my-service
    annotations:
        service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"
[...]
```

{{% /tab %}} {{% tab name="Tencent Cloud" %}}

```yaml
[...]
metadata:
  annotations:
    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxx
[...]
```

{{% /tab %}} {{< /tabs >}}

#### Prise en charge TLS sur AWS {#ssl-support-on-aws}

Pour une prise en charge partielle de TLS / SSL sur des clusters ex√©cut√©s sur
AWS, vous pouvez ajouter trois annotations √† un service `LoadBalancer`:

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012
```

Le premier sp√©cifie l'ARN du certificat √† utiliser. Il peut s'agir soit d'un
certificat d'un √©metteur tiers qui a √©t√© t√©l√©charg√© sur IAM, soit d'un
certificat cr√©√© dans AWS Certificate Manager.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp)
```

La deuxi√®me annotation sp√©cifie le protocole utilis√© par un pod. Pour HTTPS et
SSL, l'ELB s'attend √† ce que le pod s'authentifie sur la connexion chiffr√©e, √
l'aide d'un certificat.

HTTP et HTTPS s√©lectionnent le proxy de couche 7: l'ELB met fin √† la connexion
avec l'utilisateur, analyse les en-t√™tes et injecte l'en-t√™te `X-Forwarded-For`
avec l'adresse IP de l'utilisateur (les pods ne voient que l'adresse IP de l'ELB
√† l'autre extr√©mit√© de sa connexion) lors du transfert des demandes.

TCP et SSL s√©lectionnent le proxy de couche 4: l'ELB transf√®re le trafic sans
modifier les en-t√™tes.

Dans un environnement √† usage mixte o√π certains ports sont s√©curis√©s et d'autres
non chiffr√©s, vous pouvez utiliser les annotations suivantes:

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443,8443"
```

Dans l'exemple ci-dessus, si le service contenait trois ports, ¬´80¬ª, ¬´443¬ª et
¬´8443¬ª, alors ¬´443¬ª et ¬´8443¬ª utiliseraient le certificat SSL, mais ¬´80¬ª serait
simplement un proxy HTTP.

A partir de Kubernetes v1.9, vous pouvez utiliser des
[strat√©gies SSL AWS pr√©d√©finies](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html)
avec des √©couteurs HTTPS ou SSL pour vos services. Pour voir quelles politiques
sont disponibles, vous pouvez utiliser l'outil de ligne de commande `aws`:

```bash
aws elb describe-load-balancer-policies --query 'PolicyDescriptions[].PolicyName'
```

Vous pouvez ensuite sp√©cifier l'une de ces strat√©gies √† l'aide de l'annotation
"`service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy`"; par
exemple:

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
```

#### Prise en charge du protocole PROXY sur AWS

Pour activer
[protocole PROXY](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)
prise en charge des clusters ex√©cut√©s sur AWS, vous pouvez utiliser l'annotation
de service suivante:

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
```

Depuis la version 1.3.0, l'utilisation de cette annotation s'applique √† tous les
ports mandat√©s par l'ELB et ne peut pas √™tre configur√©e autrement.

#### Journaux d'acc√®s ELB sur AWS

Il existe plusieurs annotations pour g√©rer les journaux d'acc√®s aux services ELB
sur AWS.

L'annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-enabled`
contr√¥le si les journaux d'acc√®s sont activ√©s.

L'annotation
`service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval` contr√¥le
l'intervalle en minutes pour la publication des journaux d'acc√®s. Vous pouvez
sp√©cifier un intervalle de 5 ou 60 minutes.

L'annotation
`service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name`
contr√¥le le nom du bucket Amazon S3 o√π les journaux d'acc√®s au load balancer
sont stock√©s.

L'annotation
`service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix`
sp√©cifie la hi√©rarchie logique que vous avez cr√©√©e pour votre bucket Amazon S3.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "true"
    # Sp√©cifie si les journaux d'acc√®s sont activ√©s pour le load balancer

    service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: "60"
    # L'intervalle de publication des journaux d'acc√®s.
    # Vous pouvez sp√©cifier un intervalle de 5 ou 60 (minutes).

    service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "my-bucket"
    # Le nom du bucket Amazon S3 o√π les journaux d'acc√®s sont stock√©s

    service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: "my-bucket-prefix/prod"
    # La hi√©rarchie logique que vous avez cr√©√©e pour votre bucket Amazon S3, par exemple `my-bucket-prefix/prod`
```

#### Drainage de connexion sur AWS

Le drainage des connexions pour les ELB classiques peut √™tre g√©r√© avec
l'annotation
`service.beta.kubernetes.io / aws-load-balancer-connection-draining-enabled`
d√©finie sur la valeur `true`. L'annotation
`service.beta.kubernetes.io / aws-load-balancer-connection-draining-timeout`
peut √©galement √™tre utilis√©e pour d√©finir la dur√©e maximale, en secondes, pour
garder les connexions existantes ouvertes avant de d√©senregistrer les instances.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "60"
```

#### Autres annotations ELB

Il existe d'autres annotations pour g√©rer les Elastic Load Balancers d√©crits
ci-dessous.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"
    # D√©lai, en secondes, pendant lequel la connexion peut √™tre inactive (aucune donn√©e n'a √©t√© envoy√©e via la connexion) avant d'√™tre ferm√©e par le load balancer

    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    # Sp√©cifie si le load balancing inter-zones est activ√© pour le load balancer

    service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: "environment=prod,owner=devops"
    # Une liste de paires cl√©-valeur s√©par√©es par des virgules qui seront enregistr√©es en tant que balises suppl√©mentaires dans l'ELB.

    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: ""
    # Nombre de contr√¥les de sant√© successifs r√©ussis requis pour qu'un backend soit consid√©r√© comme sain pour le trafic.
    # La valeur par d√©faut est 2, doit √™tre comprise entre 2 et 10

    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "3"
    # Nombre de contr√¥les de sant√© infructueux requis pour qu'un backend soit consid√©r√© comme inapte pour le trafic.
    # La valeur par d√©faut est 6, doit √™tre comprise entre 2 et 10

    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "20"
    # Intervalle approximatif, en secondes, entre les contr√¥les d'int√©grit√© d'une instance individuelle.
    # La valeur par d√©faut est 10, doit √™tre comprise entre 5 et 300

    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "5"
    # Dur√©e, en secondes, pendant laquelle aucune r√©ponse ne signifie l'√©chec d'un contr√¥le de sant√©.
    # Cette valeur doit √™tre inf√©rieure √† la valeur service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval.
    # La valeur par d√©faut est 5, doit √™tre comprise entre 2 et 60

    service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: "sg-53fae93f,sg-42efd82e"
    # Une liste de groupes de s√©curit√© suppl√©mentaires √† ajouter √† l'ELB
```

#### Prise en charge du load balancer r√©seau sur AWS {#aws-nlb-support}

{{< feature-state for_k8s_version="v1.15" state="beta" >}}

Pour utiliser un load balancer r√©seau sur AWS, utilisez l'annotation
`service.beta.kubernetes.io/aws-load-balancer-type` avec la valeur d√©finie sur
`nlb`.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
```

{{< note >}} NLB ne fonctionne qu'avec certaines classes d'instance; voir la
[documentation AWS](http://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets)
sur Elastic Load Balancing pour une liste des types d'instances pris en charge.
{{< /note >}}

Contrairement aux √©quilibreurs de charge √©lastiques classiques, les √©quilibreurs
de charge r√©seau (NLB) transf√®rent l'adresse IP du client jusqu'au n≈ìud. Si un
service est `.spec.externalTrafficPolicy` est r√©gl√© sur `Cluster`, l'adresse IP
du client n'est pas propag√©e aux pods finaux.

En d√©finissant `.spec.externalTrafficPolicy` √† `Local`, les adresses IP des
clients sont propag√©es aux pods finaux, mais cela peut entra√Æner une r√©partition
in√©gale du trafic. Les n≈ìuds sans pods pour un service LoadBalancer particulier
√©choueront au contr√¥le de sant√© du groupe cible NLB sur le
`.spec.healthCheckNodePort` attribu√© automatiquement et ne recevront aucun
trafic.

Pour obtenir un trafic uniforme, utilisez un DaemonSet ou sp√©cifiez un
[pod anti-affinity](/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity)
pour ne pas localiser sur le m√™me noeud.

Vous pouvez √©galement utiliser les services NLB avec l'annotation
[load balancer internal](/docs/concepts/services-networking/service/#internal-load-balancer).

Pour que le trafic client atteigne des instances derri√®re un NLB, les groupes de
s√©curit√© du n≈ìud sont modifi√©s avec les r√®gles IP suivantes:

| Rule           | Protocol | Port(s)                                                                             | IpRange(s)                                                 | IpRange Description                                |
| -------------- | -------- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------- |
| Health Check   | TCP      | NodePort(s) (`.spec.healthCheckNodePort` for `.spec.externalTrafficPolicy = Local`) | VPC CIDR                                                   | kubernetes.io/rule/nlb/health=\<loadBalancerName\> |
| Client Traffic | TCP      | NodePort(s)                                                                         | `.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`) | kubernetes.io/rule/nlb/client=\<loadBalancerName\> |
| MTU Discovery  | ICMP     | 3,4                                                                                 | `.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`) | kubernetes.io/rule/nlb/mtu=\<loadBalancerName\>    |

Afin de limiter les IP clientes pouvant acc√©der √† l'√©quilibreur de charge
r√©seau, sp√©cifiez `loadBalancerSourceRanges`.

```yaml
spec:
  loadBalancerSourceRanges:
    - "143.231.0.0/16"
```

{{< note >}} Si `.spec.loadBalancerSourceRanges` n'est pas d√©fini, Kubernetes
autorise le trafic de `0.0.0.0/0` vers les groupes de s√©curit√© des n≈ìuds. Si les
n≈ìuds ont des adresses IP publiques, sachez que le trafic non NLB peut √©galement
atteindre toutes les instances de ces groupes de s√©curit√© modifi√©s.

{{< /note >}}

#### Autres annotations CLB sur Tencent Kubernetes Engine (TKE)

Il existe d'autres annotations pour la gestion des √©quilibreurs de charge cloud
sur TKE, comme indiqu√© ci-dessous.

```yaml
    metadata:
      name: my-service
      annotations:
        # Lier des load balancers avec des n≈ìuds sp√©cifiques
        service.kubernetes.io/qcloud-loadbalancer-backends-label: key in (value1, value2)

        # ID d'un load balancer existant
        service.kubernetes.io/tke-existed-lbidÔºölb-6swtxxxx

        # Param√®tres personnalis√©s pour le load balancer (LB), ne prend pas encore en charge la modification du type LB
        service.kubernetes.io/service.extensiveParameters: ""

        # Param√®tres personnalis√©s pour le listener LB
        service.kubernetes.io/service.listenerParameters: ""

        # Sp√©cifie le type de Load balancer;
        # valeurs valides: classic (Classic Cloud Load Balancer) ou application (Application Cloud Load Balancer)
        service.kubernetes.io/loadbalance-type: xxxxx

        # Sp√©cifie la m√©thode de facturation de la bande passante du r√©seau public;
        # valid values: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) and BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).
        service.kubernetes.io/qcloud-loadbalancer-internet-charge-type: xxxxxx

        # Sp√©cifie la valeur de bande passante (plage de valeurs: [1,2000] Mbps).
        service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out: "10"

        # Lorsque cette annotation est d√©finie, les √©quilibreurs de charge n'enregistrent que les n≈ìuds sur lesquels le pod s'ex√©cute, sinon tous les n≈ìuds seront enregistr√©s.
        service.kubernetes.io/local-svc-only-bind-node-with-pod: true
```

### Type ExternalName {#externalname}

Les services de type ExternalName mappent un service √† un nom DNS, et non √† un
s√©lecteur standard tel que `my-service` ou `cassandra`. Vous sp√©cifiez ces
services avec le param√®tre `spec.externalName`.

Cette d√©finition de service, par exemple, mappe le service `my-service` dans
l'espace de noms`prod` √† `my.database.example.com`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
```

{{< note >}} ExternalName accepte une cha√Æne d'adresse IPv4, mais en tant que
noms DNS compos√©s de chiffres, et non en tant qu'adresse IP. Les noms externes
qui ressemblent aux adresses IPv4 ne sont pas r√©solus par CoreDNS ou
ingress-nginx car ExternalName est destin√© √† sp√©cifier un nom DNS canonique.
Pour coder en dur une adresse IP, pensez √† utiliser des
[Services headless](#headless-services). {{< /note >}}

Lors de la recherche de l'h√¥te `my-service.prod.svc.cluster.local`, le service
DNS du cluster renvoie un enregistrement`CNAME` avec la valeur
`my.database.example.com`. L'acc√®s √† ¬´mon-service¬ª fonctionne de la m√™me mani√®re
que les autres services, mais avec la diff√©rence cruciale que la redirection se
produit au niveau DNS plut√¥t que via un proxy ou un transfert. Si vous d√©cidez
ult√©rieurement de d√©placer votre base de donn√©es dans votre cluster, vous pouvez
d√©marrer ses pods, ajouter des s√©lecteurs ou des Endpoints appropri√©s et
modifier le `type` du service.

{{< warning >}} Vous pouvez rencontrer des difficult√©s √† utiliser ExternalName
pour certains protocoles courants, notamment HTTP et HTTPS. Si vous utilisez
ExternalName, le nom d'h√¥te utilis√© par les clients √† l'int√©rieur de votre
cluster est diff√©rent du nom r√©f√©renc√© par ExternalName.

Pour les protocoles qui utilisent des noms d'h√¥tes, cette diff√©rence peut
entra√Æner des erreurs ou des r√©ponses inattendues. Les requ√™tes HTTP auront un
en-t√™te `Host:` que le serveur d'origine ne reconna√Æt pas; Les serveurs TLS ne
pourront pas fournir de certificat correspondant au nom d'h√¥te auquel le client
s'est connect√©. {{< /warning >}}

{{< note >}} Cette section est redevable √† l'article
[Kubernetes Tips - Part 1](https://akomljen.com/kubernetes-tips-part-1/)
d'[Alen Komljen](https://akomljen.com/). {{< /note >}}

### IP externes

S'il existe des adresses IP externes qui acheminent vers un ou plusieurs n≈ìuds
de cluster, les services Kubernetes peuvent √™tre expos√©s sur ces "IP externes".
Le trafic qui p√©n√®tre dans le cluster avec l'IP externe (en tant qu'IP de
destination), sur le port de service, sera rout√© vers l'un des Endpoints de
service. Les `externalIPs` ne sont pas g√©r√©es par Kubernetes et rel√®vent de la
responsabilit√© de l'administrateur du cluster.

Dans la sp√©cification de service, ¬´externalIPs¬ª peut √™tre sp√©cifi√© avec
n'importe lequel des ¬´ServiceTypes¬ª. Dans l'exemple ci-dessous, "`my-service`"
peut √™tre consult√© par les clients sur "`80.11.12.10:80`" (`externalIP:port`)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10
```

## Lacunes

Le proxy fonctionnant dans l'espace utilisateur pour les VIP peut fonctionner √
petite ou moyenne √©chelle, mais montrera ses limites dans de tr√®s grands
clusters avec des milliers de services. La
[proposition de conception originale pour les portails](http://issue.k8s.io/1107)
a plus de d√©tails √† ce sujet.

L'utilisation du proxy de l'espace utilisateur masque l'adresse IP source d'un
paquet acc√©dant √† un service. Cela rend certains types de filtrage r√©seau
(pare-feu) impossibles. Le mode proxy iptables n'obscurcit pas les adresses IP
source dans le cluster, mais il affecte toujours les clients passant par un
`LoadBalancer` ou un `NodePort`.

Le champ `Type` est con√ßu comme une fonctionnalit√© imbriqu√©e - chaque niveau
s'ajoute au pr√©c√©dent. Cela n'est pas strictement requis sur tous les
fournisseurs de cloud (par exemple, Google Compute Engine n'a pas besoin
d'allouer un `NodePort` pour faire fonctionner `LoadBalancer`, mais AWS le fait)
mais l'API actuelle le requiert.

## Impl√©mentation IP virtuelle {#the-gory-details-of-virtual-ips}

Les informations pr√©c√©dentes devraient √™tre suffisantes pour de nombreuses
personnes qui souhaitent simplement utiliser les Services. Cependant, il se
passe beaucoup de choses dans les coulisses qui m√©ritent d'√™tre comprises.

### √âviter les collisions

L'une des principales philosophies de Kubernetes est que vous ne devez pas √™tre
expos√© √† des situations qui pourraient entra√Æner l'√©chec de vos actions sans
aucune faute de votre part. Pour la conception de la ressource Service, cela
signifie de ne pas vous faire choisir votre propre num√©ro de port si ce choix
pourrait entrer en collision avec le choix de quelqu'un d'autre. C'est un √©chec
d'isolement.

Afin de vous permettre de choisir un num√©ro de port pour vos Services, nous
devons nous assurer qu'aucun deux Services ne peuvent entrer en collision.
Kubernetes le fait en attribuant √† chaque service sa propre adresse IP.

Pour garantir que chaque service re√ßoit une adresse IP unique, un allocateur
interne met √† jour atomiquement une carte d'allocation globale dans
{{< glossary_tooltip term_id="etcd" >}} avant de cr√©er chaque service. L'objet
de mappage doit exister dans le registre pour que les services obtiennent des
affectations d'adresse IP, sinon les cr√©ations √©choueront avec un message
indiquant qu'une adresse IP n'a pas pu √™tre allou√©e.

Dans le plan de contr√¥le, un contr√¥leur d'arri√®re-plan est responsable de la
cr√©ation de cette carte (n√©cessaire pour prendre en charge la migration √† partir
d'anciennes versions de Kubernetes qui utilisaient le verrouillage en m√©moire).
Kubernetes utilise √©galement des contr√¥leurs pour v√©rifier les affectations non
valides (par exemple en raison d'une intervention de l'administrateur) et pour
nettoyer les adresses IP allou√©es qui ne sont plus utilis√©es par aucun service.

### Service IP addresses {#ips-and-vips}

Contrairement aux adresses IP des pods, qui acheminent r√©ellement vers une
destination fixe, les adresses IP des services ne sont pas r√©ellement r√©pondues
par un seul h√¥te. Au lieu de cela, kube-proxy utilise iptables (logique de
traitement des paquets sous Linux) pour d√©finir les adresses IP _virtual_ qui
sont redirig√©es de mani√®re transparente selon les besoins. Lorsque les clients
se connectent au VIP, leur trafic est automatiquement transport√© vers un
Endpoint appropri√©. Les variables d'environnement et DNS pour les services sont
en fait remplis en termes d'adresse IP virtuelle (et de port) du service.

kube-proxy prend en charge trois modes proxy &mdash; espace utilisateur,
iptables et IPVS &mdash; qui fonctionnent chacun l√©g√®rement diff√©remment.

#### Userspace

√Ä titre d'exemple, consid√©rons l'application de traitement d'image d√©crite
ci-dessus. Lorsque le service backend est cr√©√©, le ma√Ætre Kubernetes attribue
une adresse IP virtuelle, par exemple 10.0.0.1. En supposant que le port de
service est 1234, le service est observ√© par toutes les instances kube-proxy
dans le cluster. Lorsqu'un proxy voit un nouveau service, il ouvre un nouveau
port al√©atoire, √©tablit une redirection iptables de l'adresse IP virtuelle vers
ce nouveau port et commence √† accepter les connexions sur celui-ci.

Lorsqu'un client se connecte √† l'adresse IP virtuelle du service, la r√®gle
iptables entre en jeu et redirige les paquets vers le propre port du proxy. Le
‚ÄúService proxy‚Äù choisit un backend, et commence le proxy du trafic du client
vers le backend.

Cela signifie que les propri√©taires de services peuvent choisir le port de leur
choix sans risque de collision. Les clients peuvent simplement se connecter √
une adresse IP et √† un port, sans savoir √† quels pods ils acc√®dent r√©ellement.

#### iptables

Consid√©rons √† nouveau l'application de traitement d'image d√©crite ci-dessus.
Lorsque le service backend est cr√©√©, le plan de contr√¥le Kubernetes attribue une
adresse IP virtuelle, par exemple 10.0.0.1. En supposant que le port de service
est 1234, le service est observ√© par toutes les instances de kube-proxy dans le
cluster. Lorsqu'un proxy voit un nouveau service, il installe une s√©rie de
r√®gles iptables qui redirigent de l'adresse IP virtuelle vers des r√®gles par
service. Les r√®gles par service sont li√©es aux r√®gles des Endpoints qui
redirigent le trafic (√† l'aide du NAT de destination) vers les backends.

Lorsqu'un client se connecte √† l'adresse IP virtuelle du service, la r√®gle
iptables entre en jeu. Un backend est choisi (soit en fonction de l'affinit√© de
la session, soit au hasard) et les paquets sont redirig√©s vers le backend.
Contrairement au proxy de l'espace utilisateur, les paquets ne sont jamais
copi√©s dans l'espace utilisateur, le proxy de kube n'a pas besoin d'√™tre ex√©cut√©
pour que l'adresse IP virtuelle fonctionne et les n≈ìuds voient le trafic
provenant de l'adresse IP du client non modifi√©e.

Ce m√™me flux de base s'ex√©cute lorsque le trafic arrive via un port de n≈ìud ou
via un load balancer, bien que dans ces cas, l'adresse IP du client soit
modifi√©e.

#### IPVS

Les op√©rations iptables ralentissent consid√©rablement dans un cluster √† grande
√©chelle, par exemple 10000 services. IPVS est con√ßu pour l'√©quilibrage de charge
et bas√© sur des tables de hachage dans le noyau. Ainsi, vous pouvez obtenir une
coh√©rence des performances dans un grand nombre de services √† partir d'un
kube-proxy bas√© sur IPVS. De plus, kube-proxy bas√© sur IPVS a des algorithmes
d'√©quilibrage de charge plus sophistiqu√©s (le moins de connexions, localit√©,
pond√©r√©, persistance).

## Objet API

Le service est une ressource de niveau sup√©rieur dans l'API REST Kubernetes.
Vous pouvez trouver plus de d√©tails sur l'objet API sur: [Service API
object](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#service-v1-core).

## Protocoles pris en charge {#protocol-support}

### TCP

{{< feature-state for_k8s_version="v1.0" state="stable" >}}

Vous pouvez utiliser TCP pour tout type de service, et c'est le protocole r√©seau
par d√©faut.

### UDP

{{< feature-state for_k8s_version="v1.0" state="stable" >}}

Vous pouvez utiliser UDP pour la plupart des services. Pour Services de type
LoadBalancer, la prise en charge UDP d√©pend du fournisseur de cloud offrant
cette fonctionnalit√©.

### HTTP

{{< feature-state for_k8s_version="v1.1" state="stable" >}}

Si votre fournisseur de cloud le prend en charge, vous pouvez utiliser un
service dans le mode LoadBalancer pour configurer le proxy inverse HTTP / HTTPS
externe, transmis au Endpoints du Service.

{{< note >}} Vous pouvez aussi utiliser
{{< glossary_tooltip term_id="ingress" >}} √† la place du service pour exposer
les services HTTP/HTTPS. {{< /note >}}

### Protocole PROXY

{{< feature-state for_k8s_version="v1.1" state="stable" >}}

Si votre fournisseur de cloud le prend en charge(eg,
[AWS](/docs/concepts/cluster-administration/cloud-providers/#aws)), vous pouvez
utiliser un service en mode LoadBalancer pour configurer un load balancer en
dehors de Kubernetes lui-m√™me, qui transmettra les connexions pr√©fix√©es par
[PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt).

Le load balancer enverra une premi√®re s√©rie d'octets d√©crivant la connexion
entrante, similaire √† cet exemple

```
PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
```

suivi des donn√©es du client.

### SCTP

{{< feature-state for_k8s_version="v1.12" state="alpha" >}}

Kubernetes prend en charge SCTP en tant que valeur de ¬´protocole¬ª dans les
d√©finitions de Service, Endpoint, NetworkPolicy et Pod en tant que
fonctionnalit√© alpha. Pour activer cette fonction, l'administrateur du cluster
doit activer le flag `SCTPSupport` sur l'apiserver, par exemple,
`--feature-gates=SCTPSupport=true,‚Ä¶`.

When the feature gate is enabled, you can set the `protocol` field of a Service,
Endpoint, NetworkPolicy or Pod to `SCTP`. Kubernetes sets up the network
accordingly for the SCTP associations, just like it does for TCP connections.

#### Avertissements {#caveat-sctp-overview}

##### Prise en charge des associations SCTP multi-h√¥tes {#caveat-sctp-multihomed}

{{< warning >}} La prise en charge des associations SCTP multi-h√¥tes n√©cessite
que le plug-in CNI puisse prendre en charge l'attribution de plusieurs
interfaces et adresses IP √† un pod.

Le NAT pour les associations SCTP multi-h√¥tes n√©cessite une logique sp√©ciale
dans les modules de noyau correspondants. {{< /warning >}}

##### Service avec type=LoadBalancer {#caveat-sctp-loadbalancer-service-type}

{{< warning >}} Vous ne pouvez cr√©er un service de type LoadBalancer avec SCTP
que si le fournisseur de load balancer supporte SCTP comme protocole. Sinon, la
demande de cr√©ation de service est rejet√©e. L'ensemble actuel de fournisseurs de
load balancer cloud (Azure, AWS, CloudStack, GCE, OpenStack) ne prennent pas en
charge SCTP. {{< /warning >}}

##### Windows {#caveat-sctp-windows-os}

{{< warning >}} SCTP n'est pas pris en charge sur les n≈ìuds Windows.
{{< /warning >}}

##### Userspace kube-proxy {#caveat-sctp-kube-proxy-userspace}

{{< warning >}} Le kube-proxy ne prend pas en charge la gestion des associations
SCTP lorsqu'il est en mode userspace. {{< /warning >}}

## Futurs d√©veloppements

√Ä l'avenir, la strat√©gie de proxy pour les services peut devenir plus nuanc√©e
que le simple √©quilibrage altern√©, par exemple master-elected ou sharded. Nous
pr√©voyons √©galement que certains services auront des load balancer ¬´r√©els¬ª,
auquel cas l'adresse IP virtuelle y transportera simplement les paquets.

Le projet Kubernetes vise √† am√©liorer la prise en charge des services L7 (HTTP).

Le projet Kubernetes pr√©voit d'avoir des modes d'entr√©e plus flexibles pour les
services, qui englobent les modes ClusterIP, NodePort et LoadBalancer actuels et
plus encore.

{{% /capture %}}

{{% capture whatsnext %}}

- Voir
  [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
- Voir [Ingress](/docs/concepts/services-networking/ingress/)
- Voir [Endpoint Slices](/docs/concepts/services-networking/endpoint-slices/)

{{% /capture %}}

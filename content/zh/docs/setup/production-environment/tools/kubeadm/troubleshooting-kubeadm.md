---
title: å¯¹ kubeadm è¿›è¡Œæ•…éšœæ’æŸ¥
content_template: templates/concept
weight: 20
---

## <!--

title: Troubleshooting kubeadm content_template: templates/concept weight: 20

---

--> {{% capture overview %}}

<!--
As with any program, you might run into an error installing or running kubeadm.
This page lists some common failure scenarios and have provided steps that can help you understand and fix the problem.

If your problem is not listed below, please follow the following steps:

- If you think your problem is a bug with kubeadm:
  - Go to [github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm/issues) and search for existing issues.
  - If no issue exists, please [open one](https://github.com/kubernetes/kubeadm/issues/new) and follow the issue template.

- If you are unsure about how kubeadm works, you can ask on [Slack](http://slack.k8s.io/) in #kubeadm, or open a question on [StackOverflow](https://stackoverflow.com/questions/tagged/kubernetes). Please include
  relevant tags like `#kubernetes` and `#kubeadm` so folks can help you.
-->

ä¸ä»»ä½•ç¨‹åºä¸€æ ·ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨å®‰è£…æˆ–è€…è¿è¡Œ kubeadm æ—¶é‡åˆ°é”™è¯¯ã€‚æœ¬æ–‡åˆ—ä¸¾äº†ä¸€äº›å¸¸è§çš„
æ•…éšœåœºæ™¯ï¼Œå¹¶æä¾›å¯å¸®åŠ©æ‚¨ç†è§£å’Œè§£å†³è¿™äº›é—®é¢˜çš„æ­¥éª¤ã€‚

å¦‚æœæ‚¨çš„é—®é¢˜æœªåœ¨ä¸‹é¢åˆ—å‡ºï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

- å¦‚æœæ‚¨è®¤ä¸ºé—®é¢˜æ˜¯ kubeadm çš„é”™è¯¯ï¼š

  - è½¬åˆ°
    [github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm/issues)
    å¹¶æœç´¢å­˜åœ¨çš„é—®é¢˜ã€‚
  - å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¯· [æ‰“å¼€](https://github.com/kubernetes/kubeadm/issues/new) å¹¶
    éµå¾ªé—®é¢˜æ¨¡æ¿ã€‚

- å¦‚æœæ‚¨å¯¹ kubeadm çš„å·¥ä½œæ–¹å¼æœ‰ç–‘é—®ï¼Œå¯ä»¥åœ¨ [Slack](http://slack.k8s.io/) ä¸Šçš„
  #kubeadm é¢‘é“æé—®ï¼Œæˆ–è€…åœ¨
  [StackOverflow](https://stackoverflow.com/questions/tagged/kubernetes) ä¸Šæé—®
  ã€‚è¯·åŠ å…¥ç›¸å…³æ ‡ç­¾ï¼Œä¾‹å¦‚ `#kubernetes` å’Œ `#kubeadm`ï¼Œè¿™æ ·å…¶ä»–äººå¯ä»¥å¸®åŠ©æ‚¨ã€‚

{{% /capture %}}

{{% capture body %}}

<!--
## `ebtables` or some similar executable not found during installation

If you see the following warnings while running `kubeadm init`

```sh
[preflight] WARNING: ebtables not found in system path
[preflight] WARNING: ethtool not found in system path
```

Then you may be missing `ebtables`, `ethtool` or a similar executable on your node. You can install them with the following commands:

- For Ubuntu/Debian users, run `apt install ebtables ethtool`.
- For CentOS/Fedora users, run `yum install ebtables ethtool`.
-->

## åœ¨å®‰è£…è¿‡ç¨‹ä¸­æ²¡æœ‰æ‰¾åˆ° `ebtables` æˆ–è€…å…¶ä»–ç±»ä¼¼çš„å¯æ‰§è¡Œæ–‡ä»¶

å¦‚æœåœ¨è¿è¡Œ `kubeadm init` å‘½ä»¤æ—¶ï¼Œé‡åˆ°ä»¥ä¸‹çš„è­¦å‘Š

```sh
[preflight] WARNING: ebtables not found in system path
[preflight] WARNING: ethtool not found in system path
```

é‚£ä¹ˆæˆ–è®¸åœ¨æ‚¨çš„èŠ‚ç‚¹ä¸Šç¼ºå¤± `ebtables`ã€`ethtool` æˆ–è€…ç±»ä¼¼çš„å¯æ‰§è¡Œæ–‡ä»¶ã€‚æ‚¨å¯ä»¥ä½¿ç”¨
ä»¥ä¸‹å‘½ä»¤å®‰è£…å®ƒä»¬ï¼š

- å¯¹äº Ubuntu/Debian ç”¨æˆ·ï¼Œè¿è¡Œ `apt install ebtables ethtool` å‘½ä»¤ã€‚
- å¯¹äº CentOS/Fedora ç”¨æˆ·ï¼Œè¿è¡Œ `yum install ebtables ethtool` å‘½ä»¤ã€‚

<!--
## kubeadm blocks waiting for control plane during installation

If you notice that `kubeadm init` hangs after printing out the following line:

```sh
[apiclient] Created API client, waiting for the control plane to become ready
```
-->

## åœ¨å®‰è£…è¿‡ç¨‹ä¸­ï¼Œkubeadm ä¸€ç›´ç­‰å¾…æ§åˆ¶å¹³é¢å°±ç»ª

å¦‚æœæ‚¨æ³¨æ„åˆ° `kubeadm init` åœ¨æ‰“å°ä»¥ä¸‹è¡ŒåæŒ‚èµ·ï¼š

```sh
[apiclient] Created API client, waiting for the control plane to become ready
```

<!--
This may be caused by a number of problems. The most common are:

- network connection problems. Check that your machine has full network connectivity before continuing.
- the default cgroup driver configuration for the kubelet differs from that used by Docker.
  Check the system log file (e.g. `/var/log/message`) or examine the output from `journalctl -u kubelet`. If you see something like the following:

  ```shell
  error: failed to run Kubelet: failed to create kubelet:
  misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"
  ```

  There are two common ways to fix the cgroup driver problem:

 1. Install Docker again following instructions
  [here](/docs/setup/production-environment/container-runtimes/#docker).

 1. Change the kubelet config to match the Docker cgroup driver manually, you can refer to
    [Configure cgroup driver used by kubelet on Master Node](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-node)

- control plane Docker containers are crashlooping or hanging. You can check this by running `docker ps` and investigating each container by running `docker logs`.
-->

è¿™å¯èƒ½æ˜¯ç”±è®¸å¤šé—®é¢˜å¼•èµ·çš„ã€‚æœ€å¸¸è§çš„æ˜¯ï¼š

- ç½‘ç»œè¿æ¥é—®é¢˜ã€‚åœ¨ç»§ç»­ä¹‹å‰ï¼Œè¯·æ£€æŸ¥æ‚¨çš„è®¡ç®—æœºæ˜¯å¦å…·æœ‰å…¨éƒ¨è”é€šçš„ç½‘ç»œè¿æ¥ã€‚
- kubelet çš„é»˜è®¤ cgroup é©±åŠ¨ç¨‹åºé…ç½®ä¸åŒäº Docker ä½¿ç”¨çš„é…ç½®ã€‚æ£€æŸ¥ç³»ç»Ÿæ—¥å¿—æ–‡ä»¶ (
  ä¾‹å¦‚ `/var/log/message`) æˆ–æ£€æŸ¥ `journalctl -u kubelet` çš„è¾“å‡ºã€‚ å¦‚æœæ‚¨çœ‹è§ä»¥
  ä¸‹å†…å®¹ï¼š

  ```shell
  error: failed to run Kubelet: failed to create kubelet:
  misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"
  ```

  æœ‰ä¸¤ç§å¸¸è§æ–¹æ³•å¯è§£å†³ cgroup é©±åŠ¨ç¨‹åºé—®é¢˜ï¼š

1.  æŒ‰ç…§ [æ­¤å¤„](/docs/setup/production-environment/container-runtimes/#docker)
    çš„è¯´æ˜å†æ¬¡å®‰è£… Dockerã€‚

1.  æ›´æ”¹ kubelet é…ç½®ä»¥æ‰‹åŠ¨åŒ¹é… Docker cgroup é©±åŠ¨ç¨‹åºï¼Œæ‚¨å¯ä»¥å‚è€ƒ
    [åœ¨ä¸»èŠ‚ç‚¹ä¸Šé…ç½® kubelet è¦ä½¿ç”¨çš„ cgroup é©±åŠ¨ç¨‹åº](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-node)

- æ§åˆ¶å¹³é¢ä¸Šçš„ Docker å®¹å™¨æŒç»­è¿›å…¥å´©æºƒçŠ¶æ€æˆ–ï¼ˆå› å…¶ä»–åŸå› ï¼‰æŒ‚èµ·ã€‚æ‚¨å¯ä»¥è¿è¡Œ
  `docker ps` å‘½ä»¤æ¥æ£€æŸ¥ä»¥åŠ `docker logs` å‘½ä»¤æ¥æ£€è§†æ¯ä¸ªå®¹å™¨çš„è¿è¡Œæ—¥å¿—ã€‚

<!--
## kubeadm blocks when removing managed containers

The following could happen if Docker halts and does not remove any Kubernetes-managed containers:

```bash
sudo kubeadm reset
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Removing kubernetes-managed containers
(block)
```

A possible solution is to restart the Docker service and then re-run `kubeadm reset`:

```bash
sudo systemctl restart docker.service
sudo kubeadm reset
```

Inspecting the logs for docker may also be useful:

```sh
journalctl -ul docker
```
-->

## å½“åˆ é™¤æ‰˜ç®¡å®¹å™¨æ—¶ kubeadm é˜»å¡

å¦‚æœ Docker åœæ­¢å¹¶ä¸”ä¸åˆ é™¤ Kubernetes æ‰€ç®¡ç†çš„æ‰€æœ‰å®¹å™¨ï¼Œå¯èƒ½å‘ç”Ÿä»¥ä¸‹æƒ…å†µï¼š

```bash
sudo kubeadm reset
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Removing kubernetes-managed containers
(block)
```

ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯é‡æ–°å¯åŠ¨ Docker æœåŠ¡ï¼Œç„¶åé‡æ–°è¿è¡Œ `kubeadm reset`ï¼š

```bash
sudo systemctl restart docker.service
sudo kubeadm reset
```

æ£€æŸ¥ docker çš„æ—¥å¿—ä¹Ÿå¯èƒ½æœ‰ç”¨ï¼š

```sh
journalctl -ul docker
```

<!--
## Pods in `RunContainerError`, `CrashLoopBackOff` or `Error` state

Right after `kubeadm init` there should not be any pods in these states.

- If there are pods in one of these states _right after_ `kubeadm init`, please open an
  issue in the kubeadm repo. `coredns` (or `kube-dns`) should be in the `Pending` state
  until you have deployed the network solution.
- If you see Pods in the `RunContainerError`, `CrashLoopBackOff` or `Error` state
  after deploying the network solution and nothing happens to `coredns` (or `kube-dns`),
  it's very likely that the Pod Network solution that you installed is somehow broken.
  You might have to grant it more RBAC privileges or use a newer version. Please file
  an issue in the Pod Network providers' issue tracker and get the issue triaged there.
- If you install a version of Docker older than 1.12.1, remove the `MountFlags=slave` option
  when booting `dockerd` with `systemd` and restart `docker`. You can see the MountFlags in `/usr/lib/systemd/system/docker.service`.
  MountFlags can interfere with volumes mounted by Kubernetes, and put the Pods in `CrashLoopBackOff` state.
  The error happens when Kubernetes does not find `var/run/secrets/kubernetes.io/serviceaccount` files.
-->

## Pods å¤„äº `RunContainerError`ã€`CrashLoopBackOff` æˆ–è€… `Error` çŠ¶æ€

åœ¨ `kubeadm init` å‘½ä»¤è¿è¡Œåï¼Œç³»ç»Ÿä¸­ä¸åº”è¯¥æœ‰ pods å¤„äºè¿™ç±»çŠ¶æ€ã€‚

- åœ¨ `kubeadm init` å‘½ä»¤æ‰§è¡Œå®Œåï¼Œå¦‚æœæœ‰ pods å¤„äºè¿™äº›çŠ¶æ€ä¹‹ä¸€ï¼Œè¯·åœ¨ kubeadm ä»“
  åº“æèµ·ä¸€ä¸ª issueã€‚`coredns` (æˆ–è€… `kube-dns`) åº”è¯¥å¤„äº `Pending` çŠ¶æ€ï¼Œç›´åˆ°æ‚¨
  éƒ¨ç½²äº†ç½‘ç»œè§£å†³æ–¹æ¡ˆä¸ºæ­¢ã€‚
- å¦‚æœåœ¨éƒ¨ç½²å®Œç½‘ç»œè§£å†³æ–¹æ¡ˆä¹‹åï¼Œæœ‰ Pods å¤„äº
  `RunContainerError`ã€`CrashLoopBackOff` æˆ– `Error` çŠ¶æ€ä¹‹ä¸€ï¼Œå¹¶ä¸”`coredns` ï¼ˆ
  æˆ–è€… `kube-dns`ï¼‰ä»å¤„äº `Pending` çŠ¶æ€ï¼Œé‚£å¾ˆå¯èƒ½æ˜¯æ‚¨å®‰è£…çš„ç½‘ç»œè§£å†³æ–¹æ¡ˆç”±äºæŸç§
  åŸå› æ— æ³•å·¥ä½œã€‚æ‚¨æˆ–è®¸éœ€è¦æˆäºˆå®ƒæ›´å¤šçš„ RBAC ç‰¹æƒæˆ–ä½¿ç”¨è¾ƒæ–°çš„ç‰ˆæœ¬ã€‚è¯·åœ¨ Pod
  Network æä¾›å•†çš„é—®é¢˜è·Ÿè¸ªå™¨ä¸­æäº¤é—®é¢˜ï¼Œç„¶ååœ¨æ­¤å¤„åˆ†ç±»é—®é¢˜ã€‚
- å¦‚æœæ‚¨å®‰è£…çš„ Docker ç‰ˆæœ¬æ—©äº 1.12.1ï¼Œè¯·åœ¨ä½¿ç”¨ `systemd` æ¥å¯åŠ¨ `dockerd` å’Œé‡
  å¯ `docker` æ—¶ï¼Œåˆ é™¤ `MountFlags=slave` é€‰é¡¹ã€‚æ‚¨å¯ä»¥åœ¨
  `/usr/lib/systemd/system/docker.service` ä¸­çœ‹åˆ° MountFlagsã€‚ MountFlags å¯èƒ½ä¼š
  å¹²æ‰° Kubernetes æŒ‚è½½çš„å·ï¼Œ å¹¶ä½¿ Pods å¤„äº `CrashLoopBackOff` çŠ¶æ€ã€‚å½“
  Kubernetes ä¸èƒ½æ‰¾åˆ° `var/run/secrets/kubernetes.io/serviceaccount` æ–‡ä»¶æ—¶ä¼šå‘
  ç”Ÿé”™è¯¯ã€‚

<!--
## `coredns` (or `kube-dns`) is stuck in the `Pending` state

This is **expected** and part of the design. kubeadm is network provider-agnostic, so the admin
should [install the pod network solution](/docs/concepts/cluster-administration/addons/)
of choice. You have to install a Pod Network
before CoreDNS may be deployed fully. Hence the `Pending` state before the network is set up.
-->

## `coredns` ï¼ˆæˆ– `kube-dns`ï¼‰åœæ»åœ¨ `Pending` çŠ¶æ€

è¿™ä¸€è¡Œä¸ºæ˜¯ **é¢„æœŸä¹‹ä¸­** çš„ï¼Œå› ä¸ºç³»ç»Ÿå°±æ˜¯è¿™ä¹ˆè®¾è®¡çš„ã€‚ kubeadm çš„ç½‘ç»œä¾›åº”å•†æ˜¯ä¸­ç«‹
çš„ï¼Œå› æ­¤ç®¡ç†å‘˜åº”è¯¥é€‰æ‹©
[å®‰è£… pod çš„ç½‘ç»œè§£å†³æ–¹æ¡ˆ](/docs/concepts/cluster-administration/addons/)ã€‚æ‚¨å¿…é¡»
å®Œæˆ Pod çš„ç½‘ç»œé…ç½®ï¼Œç„¶åæ‰èƒ½å®Œå…¨éƒ¨ç½² CoreDNSã€‚åœ¨ç½‘ç»œè¢«é…ç½®å¥½ä¹‹å‰ï¼ŒDNS ç»„ä»¶ä¼šä¸€
ç›´å¤„äº `Pending` çŠ¶æ€ã€‚

<!--
## `HostPort` services do not work

The `HostPort` and `HostIP` functionality is available depending on your Pod Network
provider. Please contact the author of the Pod Network solution to find out whether
`HostPort` and `HostIP` functionality are available.

Calico, Canal, and Flannel CNI providers are verified to support HostPort.

For more information, see the [CNI portmap documentation](https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md).

If your network provider does not support the portmap CNI plugin, you may need to use the [NodePort feature of
services](/docs/concepts/services-networking/service/#nodeport) or use `HostNetwork=true`.
-->

## `HostPort` æœåŠ¡æ— æ³•å·¥ä½œ

æ­¤ `HostPort` å’Œ `HostIP` åŠŸèƒ½æ˜¯å¦å¯ç”¨å–å†³äºæ‚¨çš„ Pod ç½‘ç»œé…ç½®ã€‚è¯·è”ç³» Pod è§£å†³æ–¹
æ¡ˆçš„ä½œè€…ï¼Œä»¥ç¡®è®¤ `HostPort` å’Œ `HostIP` åŠŸèƒ½æ˜¯å¦å¯ç”¨ã€‚

å·²éªŒè¯ Calicoã€Canal å’Œ Flannel CNI é©±åŠ¨ç¨‹åºæ”¯æŒ HostPortã€‚

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ
[CNI portmap æ–‡æ¡£](https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md).

å¦‚æœæ‚¨çš„ç½‘ç»œæä¾›å•†ä¸æ”¯æŒ portmap CNI æ’ä»¶ï¼Œæ‚¨æˆ–è®¸éœ€è¦ä½¿ç”¨
[NodePort æœåŠ¡çš„åŠŸèƒ½](/docs/concepts/services-networking/service/#nodeport) æˆ–è€…
ä½¿ç”¨ `HostNetwork=true`ã€‚

<!--
## Pods are not accessible via their Service IP

- Many network add-ons do not yet enable [hairpin mode](/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip)
  which allows pods to access themselves via their Service IP. This is an issue related to
  [CNI](https://github.com/containernetworking/cni/issues/476). Please contact the network
  add-on provider to get the latest status of their support for hairpin mode.

- If you are using VirtualBox (directly or via Vagrant), you will need to
  ensure that `hostname -i` returns a routable IP address. By default the first
  interface is connected to a non-routable host-only network. A work around
  is to modify `/etc/hosts`, see this [Vagrantfile](https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11)
  for an example.
-->

## æ— æ³•é€šè¿‡å…¶æœåŠ¡ IP è®¿é—® Pod

- è®¸å¤šç½‘ç»œé™„åŠ ç»„ä»¶å°šæœªå¯ç”¨
  [hairpin æ¨¡å¼](/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip)
  è¯¥æ¨¡å¼å…è®¸ Pod é€šè¿‡å…¶æœåŠ¡ IP è¿›è¡Œè®¿é—®ã€‚è¿™æ˜¯ä¸
  [CNI](https://github.com/containernetworking/cni/issues/476) æœ‰å…³çš„é—®é¢˜ã€‚è¯·ä¸
  ç½‘ç»œé™„åŠ ç»„ä»¶æä¾›å•†è”ç³»ï¼Œä»¥è·å–ä»–ä»¬æ‰€æä¾›çš„ hairpin æ¨¡å¼çš„æœ€æ–°çŠ¶æ€ã€‚

- å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ VirtualBox (ç›´æ¥ä½¿ç”¨æˆ–è€…é€šè¿‡ Vagrant ä½¿ç”¨)ï¼Œæ‚¨éœ€è¦ç¡®ä¿
  `hostname -i` è¿”å›ä¸€ä¸ªå¯è·¯ç”±çš„ IP åœ°å€ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªæ¥å£è¿æ¥ä¸èƒ½è·¯ç”±çš„ä»…
  ä¸»æœºç½‘ç»œã€‚è§£å†³æ–¹æ³•æ˜¯ä¿®æ”¹ `/etc/hosts`ï¼Œè¯·å‚è€ƒç¤ºä¾‹
  [Vagrantfile](https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11)ã€‚

<!--
## TLS certificate errors

The following error indicates a possible certificate mismatch.

```none
# kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
```

- Verify that the `$HOME/.kube/config` file contains a valid certificate, and
  regenerate a certificate if necessary. The certificates in a kubeconfig file
  are base64 encoded. The `base64 -d` command can be used to decode the certificate
  and `openssl x509 -text -noout` can be used for viewing the certificate information.
- Unset the `KUBECONFIG` environment variable using:

  ```sh
  unset KUBECONFIG
  ```

  Or set it to the default `KUBECONFIG` location:

  ```sh
  export KUBECONFIG=/etc/kubernetes/admin.conf
  ```

- Another workaround is to overwrite the existing `kubeconfig` for the "admin" user:

  ```sh
  mv  $HOME/.kube $HOME/.kube.bak
  mkdir $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```
-->

## TLS è¯ä¹¦é”™è¯¯

ä»¥ä¸‹é”™è¯¯æŒ‡å‡ºè¯ä¹¦å¯èƒ½ä¸åŒ¹é…ã€‚

```none
# kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
```

- éªŒè¯ `$HOME/.kube/config` æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆè¯ä¹¦ï¼Œå¹¶åœ¨å¿…è¦æ—¶é‡æ–°ç”Ÿæˆè¯ä¹¦ã€‚åœ¨
  kubeconfig æ–‡ä»¶ä¸­çš„è¯ä¹¦æ˜¯ base64 ç¼–ç çš„ã€‚è¯¥ `base64 -d` å‘½ä»¤å¯ä»¥ç”¨æ¥è§£ç è¯ä¹¦
  ï¼Œ`openssl x509 -text -noout` å‘½ä»¤å¯ä»¥ç”¨äºæŸ¥çœ‹è¯ä¹¦ä¿¡æ¯ã€‚
- ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•å–æ¶ˆè®¾ç½® `KUBECONFIG` ç¯å¢ƒå˜é‡çš„å€¼ï¼š

  ```sh
  unset KUBECONFIG
  ```

  æˆ–è€…å°†å…¶è®¾ç½®ä¸ºé»˜è®¤çš„ `KUBECONFIG` ä½ç½®ï¼š

  ```sh
  export KUBECONFIG=/etc/kubernetes/admin.conf
  ```

- å¦ä¸€ä¸ªæ–¹æ³•æ˜¯è¦†ç›– `kubeconfig` çš„ç°æœ‰ç”¨æˆ· "ç®¡ç†å‘˜" ï¼š

  ```sh
  mv  $HOME/.kube $HOME/.kube.bak
  mkdir $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```

<!--
## Default NIC When using flannel as the pod network in Vagrant

The following error might indicate that something was wrong in the pod network:

```sh
Error from server (NotFound): the server could not find the requested resource
```

- If you're using flannel as the pod network inside Vagrant, then you will have to specify the default interface name for flannel.

  Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address `10.0.2.15`, is for external traffic that gets NATed.

  This may lead to problems with flannel, which defaults to the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this, pass the `--iface eth1` flag to flannel so that the second interface is chosen.
-->

## åœ¨ Vagrant ä¸­ä½¿ç”¨ flannel ä½œä¸º pod ç½‘ç»œæ—¶çš„é»˜è®¤ NIC

ä»¥ä¸‹é”™è¯¯å¯èƒ½è¡¨æ˜ Pod ç½‘ç»œä¸­å‡ºç°é—®é¢˜ï¼š

```sh
Error from server (NotFound): the server could not find the requested resource
```

- å¦‚æœä½ æ­£åœ¨ Vagrant ä¸­ä½¿ç”¨ flannel ä½œä¸º pod ç½‘ç»œï¼Œåˆ™å¿…é¡»æŒ‡å®š flannel çš„é»˜è®¤æ¥å£
  åç§°ã€‚

  Vagrant é€šå¸¸ä¸ºæ‰€æœ‰ VM åˆ†é…ä¸¤ä¸ªæ¥å£ã€‚ç¬¬ä¸€ä¸ªä¸ºæ‰€æœ‰ä¸»æœºåˆ†é…äº† IP åœ°å€
  `10.0.2.15`ï¼Œç”¨äºè·å¾— NATed çš„å¤–éƒ¨æµé‡ã€‚

  è¿™å¯èƒ½ä¼šå¯¼è‡´ flannel å‡ºç°é—®é¢˜ï¼Œå®ƒé»˜è®¤ä¸ºä¸»æœºä¸Šçš„ç¬¬ä¸€ä¸ªæ¥å£ã€‚è¿™å¯¼è‡´æ‰€æœ‰ä¸»æœºè®¤ä¸º
  å®ƒä»¬å…·æœ‰ç›¸åŒçš„å…¬å…± IP åœ°å€ã€‚ä¸ºé˜²æ­¢è¿™ç§æƒ…å†µï¼Œä¼ é€’ `--iface eth1` æ ‡å¿—ç»™ flannel
  ä»¥ä¾¿é€‰æ‹©ç¬¬äºŒä¸ªæ¥å£ã€‚

<!--
## Non-public IP used for containers

In some situations `kubectl logs` and `kubectl run` commands may return with the following errors in an otherwise functional cluster:

```sh
Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
```

- This may be due to Kubernetes using an IP that can not communicate with other IPs on the seemingly same subnet, possibly by policy of the machine provider.
- Digital Ocean assigns a public IP to `eth0` as well as a private one to be used internally as anchor for their floating IP feature, yet `kubelet` will pick the latter as the node's `InternalIP` instead of the public one.

  Use `ip addr show` to check for this scenario instead of `ifconfig` because `ifconfig` will not display the offending alias IP address. Alternatively an API endpoint specific to Digital Ocean allows to query for the anchor IP from the droplet:

  ```sh
  curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
  ```

  The workaround is to tell `kubelet` which IP to use using `--node-ip`. When using Digital Ocean, it can be the public one (assigned to `eth0`) or the private one (assigned to `eth1`) should you want to use the optional private network. The [`KubeletExtraArgs` section of the kubeadm `NodeRegistrationOptions` structure](https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go) can be used for this.

  Then restart `kubelet`:

  ```sh
  systemctl daemon-reload
  systemctl restart kubelet
  ```
-->

## å®¹å™¨ä½¿ç”¨çš„éå…¬å…± IP

åœ¨æŸäº›æƒ…å†µä¸‹ `kubectl logs` å’Œ `kubectl run` å‘½ä»¤æˆ–è®¸ä¼šè¿”å›ä»¥ä¸‹é”™è¯¯ï¼Œå³ä¾¿é™¤æ­¤ä¹‹
å¤–é›†ç¾¤ä¸€åˆ‡åŠŸèƒ½æ­£å¸¸ï¼š

```sh
Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
```

- è¿™æˆ–è®¸æ˜¯ç”±äº Kubernetes ä½¿ç”¨çš„ IP æ— æ³•ä¸çœ‹ä¼¼ç›¸åŒçš„å­ç½‘ä¸Šçš„å…¶ä»– IP è¿›è¡Œé€šä¿¡çš„ç¼˜
  æ•…ï¼Œå¯èƒ½æ˜¯ç”±æœºå™¨æä¾›å•†çš„æ”¿ç­–æ‰€å¯¼è‡´çš„ã€‚
- Digital Ocean æ—¢åˆ†é…ä¸€ä¸ªå…±æœ‰ IP ç»™ `eth0`ï¼Œä¹Ÿåˆ†é…ä¸€ä¸ªç§æœ‰ IP åœ¨å†…éƒ¨ç”¨ä½œå…¶æµ®åŠ¨
  IP åŠŸèƒ½çš„é”šç‚¹ï¼Œç„¶è€Œ `kubelet` å°†é€‰æ‹©åè€…ä½œä¸ºèŠ‚ç‚¹çš„ `InternalIP` è€Œä¸æ˜¯å…¬å…± IP

  ä½¿ç”¨ `ip addr show` å‘½ä»¤ä»£æ›¿ `ifconfig` å‘½ä»¤å»æ£€æŸ¥è¿™ç§æƒ…å†µï¼Œå› ä¸º `ifconfig` å‘½
  ä»¤ ä¸ä¼šæ˜¾ç¤ºæœ‰é—®é¢˜çš„åˆ«å IP åœ°å€ã€‚æˆ–è€…æŒ‡å®šçš„ Digital Ocean çš„ API ç«¯å£å…è®¸ä»
  droplet ä¸­ æŸ¥è¯¢ anchor IPï¼š

  ```sh
  curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
  ```

  è§£å†³æ–¹æ³•æ˜¯é€šçŸ¥ `kubelet` ä½¿ç”¨å“ªä¸ª `--node-ip`ã€‚å½“ä½¿ç”¨ Digital Ocean æ—¶ï¼Œå¯ä»¥æ˜¯
  å…¬ç½‘ IPï¼ˆåˆ†é…ç»™ `eth0`çš„ï¼‰ï¼Œ æˆ–è€…æ˜¯ç§ç½‘ IPï¼ˆåˆ†é…ç»™ `eth1` çš„ï¼‰ã€‚ç§ç½‘ IP æ˜¯å¯é€‰
  çš„ã€‚ è¿™ä¸ª
  [`KubeletExtraArgs` section of the kubeadm `NodeRegistrationOptions` structure](https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go)
  è¢«ç”¨æ¥å¤„ç†è¿™ç§æƒ…å†µã€‚

  ç„¶åé‡å¯ `kubelet`ï¼š

  ```sh
  systemctl daemon-reload
  systemctl restart kubelet
  ```

  <!--

## `coredns` pods have `CrashLoopBackOff` or `Error` state

If you have nodes that are running SELinux with an older version of Docker you
might experience a scenario where the `coredns` pods are not starting. To solve
that you can try one of the following options:

- Upgrade to a
  [newer version of Docker](/docs/setup/production-environment/container-runtimes/#docker).

- [Disable SELinux](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux).
- Modify the `coredns` deployment to set `allowPrivilegeEscalation` to `true`:

```bash
kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
```

Another cause for CoreDNS to have `CrashLoopBackOff` is when a CoreDNS Pod
deployed in Kubernetes detects a loop.
[A number of workarounds](https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters)
are available to avoid Kubernetes trying to restart the CoreDNS Pod every time
CoreDNS detects the loop and exits. -->

## `coredns` pods æœ‰ `CrashLoopBackOff` æˆ–è€… `Error` çŠ¶æ€

å¦‚æœæœ‰äº›èŠ‚ç‚¹è¿è¡Œçš„æ˜¯æ—§ç‰ˆæœ¬çš„ Dockerï¼ŒåŒæ—¶å¯ç”¨äº† SELinuxï¼Œæ‚¨æˆ–è®¸ä¼šé‡åˆ° `coredns`
pods æ— æ³•å¯åŠ¨çš„æƒ…å†µã€‚è¦è§£å†³æ­¤é—®é¢˜ï¼Œæ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š

- å‡çº§åˆ°
  [Docker çš„è¾ƒæ–°ç‰ˆæœ¬](/docs/setup/production-environment/container-runtimes/#docker)ã€‚

- [ç¦ç”¨ SELinux](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux).
- ä¿®æ”¹ `coredns` éƒ¨ç½²ä»¥è®¾ç½® `allowPrivilegeEscalation` ä¸º `true`ï¼š

```bash
kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
```

CoreDNS å¤„äº `CrashLoopBackOff` æ—¶çš„å¦ä¸€ä¸ªåŸå› æ˜¯å½“ Kubernetes ä¸­éƒ¨ç½²çš„ CoreDNS
Pod æ£€æµ‹åˆ°ç¯è·¯æ—¶
ã€‚[æœ‰è®¸å¤šè§£å†³æ–¹æ³•](https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters)
å¯ä»¥é¿å…åœ¨æ¯æ¬¡ CoreDNS ç›‘æµ‹åˆ°å¾ªç¯å¹¶é€€å‡ºæ—¶ï¼ŒKubernetes å°è¯•é‡å¯ CoreDNS Pod çš„æƒ…
å†µã€‚

{{< warning >}}

<!--
Disabling SELinux or setting `allowPrivilegeEscalation` to `true` can compromise
the security of your cluster.
-->

**è­¦å‘Š**ï¼šç¦ç”¨ SELinux æˆ–è®¾ç½® `allowPrivilegeEscalation` ä¸º `true` å¯èƒ½ä¼šæŸå®³é›†
ç¾¤çš„å®‰å…¨æ€§ã€‚ {{< /warning >}}

<!--
## etcd pods restart continually

If you encounter the following error:

```
rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused "process_linux.go:110: decoding init error from pipe caused \"read parent: connection reset by peer\""
```

this issue appears if you run CentOS 7 with Docker 1.13.1.84.
This version of Docker can prevent the kubelet from executing into the etcd container.

To work around the issue, choose one of these options:

- Roll back to an earlier version of Docker, such as 1.13.1-75
```
yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
```

- Install one of the more recent recommended versions, such as 18.06:
```bash
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
```
-->

## etcd pods æŒç»­é‡å¯

å¦‚æœæ‚¨é‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š

```
rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused "process_linux.go:110: decoding init error from pipe caused \"read parent: connection reset by peer\""
```

å¦‚æœæ‚¨ä½¿ç”¨ Docker 1.13.1.84 è¿è¡Œ CentOS 7 å°±ä¼šå‡ºç°è¿™ç§é—®é¢˜ã€‚æ­¤ç‰ˆæœ¬çš„ Docker ä¼šé˜»
æ­¢ kubelet åœ¨ etcd å®¹å™¨ä¸­æ‰§è¡Œã€‚

ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œè¯·é€‰æ‹©ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š

- å›æ»šåˆ°æ—©æœŸç‰ˆæœ¬çš„ Dockerï¼Œä¾‹å¦‚ 1.13.1-75

```
yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
```

- å®‰è£…è¾ƒæ–°çš„æ¨èç‰ˆæœ¬ä¹‹ä¸€ï¼Œä¾‹å¦‚ 18.06:

```bash
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
```

<!--
## Not possible to pass a comma separated list of values to arguments inside a `--component-extra-args` flag

`kubeadm init` flags such as `--component-extra-args` allow you to pass custom arguments to a control-plane
component like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing
the values (`mapStringString`).

If you decide to pass an argument that supports multiple, comma-separated values such as
`--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"` this flag will fail with
`flag: malformed pair, expect string=string`. This happens because the list of arguments for
`--apiserver-extra-args` expects `key=value` pairs and in this case `NamespacesExists` is considered
as a key that is missing a value.

Alternatively, you can try separating the `key=value` pairs like so:
`--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"`
but this will result in the key `enable-admission-plugins` only having the value of `NamespaceExists`.

A known workaround is to use the kubeadm [configuration file](/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags).
-->

## æ— æ³•å°†ä»¥é€—å·åˆ†éš”çš„å€¼åˆ—è¡¨ä¼ é€’ç»™ `--component-extra-args` æ ‡å¿—å†…çš„å‚æ•°

`kubeadm init` æ ‡å¿—ä¾‹å¦‚ `--component-extra-args` å…è®¸æ‚¨å°†è‡ªå®šä¹‰å‚æ•°ä¼ é€’ç»™åƒ
kube-apiserver è¿™æ ·çš„æ§åˆ¶å¹³é¢ç»„ä»¶ã€‚ç„¶è€Œï¼Œç”±äºè§£æ (`mapStringString`) çš„åŸºç¡€ç±»å‹
å€¼ï¼Œæ­¤æœºåˆ¶å°†å—åˆ°é™åˆ¶ã€‚

å¦‚æœæ‚¨å†³å®šä¼ é€’ä¸€ä¸ªæ”¯æŒå¤šä¸ªé€—å·åˆ†éš”å€¼ï¼ˆä¾‹å¦‚
`--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"`ï¼‰
å‚æ•°ï¼Œå°†å‡ºç° `flag: malformed pair, expect string=string` é”™è¯¯ã€‚å‘ç”Ÿè¿™ç§é—®é¢˜æ˜¯å›
ä¸ºå‚æ•°åˆ—è¡¨ `--apiserver-extra-args` é¢„æœŸçš„æ˜¯ `key=value` å½¢å¼ï¼Œè€Œè¿™é‡Œçš„
`NamespacesExists` è¢«è¯¯è®¤ä¸ºæ˜¯ç¼ºå°‘å–å€¼çš„é”®åã€‚

ä¸€ç§è§£å†³æ–¹æ³•æ˜¯å°è¯•åˆ†ç¦» `key=value` å¯¹ï¼Œåƒè¿™æ ·ï¼š
`--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"`
ä½†è¿™å°†å¯¼è‡´é”® `enable-admission-plugins` ä»…æœ‰å€¼ `NamespaceExists`ã€‚

å·²çŸ¥çš„è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨ kubeadm
[é…ç½®æ–‡ä»¶](/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags)ã€‚

<!--
## kube-proxy scheduled before node is initialized by cloud-controller-manager

In cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before
the cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail
to pick up the node's IP address properly and has knock-on effects to the proxy function managing
load balancers.

The following error can be seen in kube-proxy Pods:
```
server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
```

A known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane
nodes regardless of their conditions, keeping it off of other nodes until their initial guarding
conditions abate:
```
kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/master" } ] } } } }'
```

The tracking issue for this problem is [here](https://github.com/kubernetes/kubeadm/issues/1027).
-->

## åœ¨èŠ‚ç‚¹è¢«äº‘æ§åˆ¶ç®¡ç†å™¨åˆå§‹åŒ–ä¹‹å‰ï¼Œkube-proxy å°±è¢«è°ƒåº¦äº†

åœ¨äº‘ç¯å¢ƒåœºæ™¯ä¸­ï¼Œå¯èƒ½å‡ºç°åœ¨äº‘æ§åˆ¶ç®¡ç†å™¨å®ŒæˆèŠ‚ç‚¹åœ°å€åˆå§‹åŒ–ä¹‹å‰ï¼Œkube-proxy å°±è¢«è°ƒ
åº¦åˆ°æ–°èŠ‚ç‚¹äº†ã€‚è¿™ä¼šå¯¼è‡´ kube-proxy æ— æ³•æ­£ç¡®è·å–èŠ‚ç‚¹çš„ IP åœ°å€ï¼Œå¹¶å¯¹ç®¡ç†è´Ÿè½½å¹³è¡¡å™¨
çš„ä»£ç†åŠŸèƒ½äº§ç”Ÿè¿é”ååº”ã€‚

åœ¨ kube-proxy Pod ä¸­å¯ä»¥çœ‹åˆ°ä»¥ä¸‹é”™è¯¯ï¼š

```
server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
```

ä¸€ç§å·²çŸ¥çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¿®è¡¥ kube-proxy DaemonSetï¼Œä»¥å…è®¸åœ¨æ§åˆ¶å¹³é¢èŠ‚ç‚¹ä¸Šè°ƒåº¦å®ƒï¼Œè€Œ
ä¸ç®¡å®ƒä»¬çš„æ¡ä»¶å¦‚ä½•ï¼Œå°†å…¶ä¸å…¶ä»–èŠ‚ç‚¹ä¿æŒéš”ç¦»ï¼Œç›´åˆ°å®ƒä»¬çš„åˆå§‹ä¿æŠ¤æ¡ä»¶æ¶ˆé™¤ï¼š

```
kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/master" } ] } } } }'
```

æ­¤é—®é¢˜çš„è·Ÿè¸ª [åœ¨è¿™é‡Œ](https://github.com/kubernetes/kubeadm/issues/1027)ã€‚

<!--
## The NodeRegistration.Taints field is omitted when marshalling kubeadm configuration

*Note: This [issue](https://github.com/kubernetes/kubeadm/issues/1358) only applies to tools that marshal kubeadm types (e.g. to a YAML configuration file). It will be fixed in kubeadm API v1beta2.*

By default, kubeadm applies the `node-role.kubernetes.io/master:NoSchedule` taint to control-plane nodes.
If you prefer kubeadm to not taint the control-plane node, and set `InitConfiguration.NodeRegistration.Taints` to an empty slice,
the field will be omitted when marshalling. When the field is omitted, kubeadm applies the default taint.

There are at least two workarounds:

1. Use the `node-role.kubernetes.io/master:PreferNoSchedule` taint instead of an empty slice. [Pods will get scheduled on masters](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/), unless other nodes have capacity.

2. Remove the taint after kubeadm init exits:
```bash
kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule-
```
-->

## NodeRegistration.Taints å­—æ®µåœ¨ç¼–ç»„ kubeadm é…ç½®æ—¶ä¸¢å¤±

_æ³¨æ„ï¼šè¿™ä¸ª [é—®é¢˜](https://github.com/kubernetes/kubeadm/issues/1358) ä»…é€‚ç”¨äºæ“
æ§ kubeadm æ•°æ®ç±»å‹çš„å·¥å…·ï¼ˆä¾‹å¦‚ï¼ŒYAML é…ç½®æ–‡ä»¶ï¼‰ã€‚å®ƒå°†åœ¨ kubeadm API v1beta2 ä¿®
å¤ã€‚_

é»˜è®¤æƒ…å†µä¸‹ï¼Œkubeadm å°† `node-role.kubernetes.io/master:NoSchedule` æ±¡ç‚¹åº”ç”¨äºæ§
åˆ¶å¹³é¢èŠ‚ç‚¹ã€‚å¦‚æœæ‚¨å¸Œæœ› kubeadm ä¸æ±¡æŸ“æ§åˆ¶å¹³é¢èŠ‚ç‚¹ï¼Œå¹¶å°†
`InitConfiguration.NodeRegistration.Taints` è®¾ç½®æˆç©ºåˆ‡ç‰‡ï¼Œåˆ™åº”åœ¨ç¼–ç»„æ—¶çœç•¥è¯¥å­—æ®µ
ã€‚å¦‚æœçœç•¥è¯¥å­—æ®µï¼Œåˆ™ kubeadm å°†åº”ç”¨é»˜è®¤æ±¡ç‚¹ã€‚

è‡³å°‘æœ‰ä¸¤ç§è§£å†³æ–¹æ³•ï¼š

1. ä½¿ç”¨ `node-role.kubernetes.io/master:PreferNoSchedule` æ±¡ç‚¹ä»£æ›¿ç©ºåˆ‡ç‰‡ã€‚é™¤éå…¶
   ä»–èŠ‚ç‚¹å…·æœ‰å®¹é‡
   ï¼Œ[å¦åˆ™å°†åœ¨ä¸»èŠ‚ç‚¹ä¸Šè°ƒåº¦ Pods](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)ã€‚

2. åœ¨ kubeadm init é€€å‡ºååˆ é™¤æ±¡ç‚¹ï¼š

```bash
kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule-
```

{{% /capture %}}

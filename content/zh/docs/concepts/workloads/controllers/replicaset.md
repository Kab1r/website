---
reviewers:
  - Kashomon
  - bprashanth
  - madhusudancs
title: ReplicaSet
content_template: templates/concept
weight: 10
---

{{% capture overview %}}

<!--
ReplicaSet is the next-generation Replication Controller. The only difference
between a _ReplicaSet_ and a
[_Replication Controller_](/docs/concepts/workloads/controllers/replicationcontroller/) right now is
the selector support. ReplicaSet supports the new set-based selector requirements
as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors)
whereas a Replication Controller only supports equality-based selector requirements.
-->

ReplicaSet æ˜¯ä¸‹ä¸€ä»£çš„ Replication Controllerã€‚ _ReplicaSet_ å’Œ
[_Replication Controller_](/docs/concepts/workloads/controllers/replicationcontroller/)
çš„å”¯ä¸€åŒºåˆ«æ˜¯é€‰æ‹©å™¨çš„æ”¯æŒã€‚ReplicaSet æ”¯æŒæ–°çš„åŸºäºé›†åˆçš„é€‰æ‹©å™¨éœ€æ±‚ï¼Œè¿™
åœ¨[æ ‡ç­¾ç”¨æˆ·æŒ‡å—](/docs/concepts/overview/working-with-objects/labels/#label-selectors)ä¸­
æœ‰æè¿°ã€‚è€Œ Replication Controller ä»…æ”¯æŒåŸºäºç›¸ç­‰é€‰æ‹©å™¨çš„éœ€æ±‚ã€‚

{{% /capture %}}

{{% capture body %}}

<!--
## How to use a ReplicaSet

Most [`kubectl`](/docs/user-guide/kubectl/) commands that support
Replication Controllers also support ReplicaSets. One exception is the
[`rolling-update`](/docs/reference/generated/kubectl/kubectl-commands#rolling-update) command. If
you want the rolling update functionality please consider using Deployments
instead. Also, the
[`rolling-update`](/docs/reference/generated/kubectl/kubectl-commands#rolling-update) command is
imperative whereas Deployments are declarative, so we recommend using Deployments
through the [`rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout) command.

While ReplicaSets can be used independently, today it's mainly used by
[Deployments](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod
creation, deletion and updates. When you use Deployments you don't have to worry
about managing the ReplicaSets that they create. Deployments own and manage
their ReplicaSets.
-->

## æ€æ ·ä½¿ç”¨ ReplicaSet

å¤§å¤šæ•°æ”¯æŒ Replication Controllers çš„[`kubectl`](/docs/user-guide/kubectl/)å‘½ä»¤
ä¹Ÿæ”¯æŒ ReplicaSetsã€‚
ä½†[`rolling-update`](/docs/reference/generated/kubectl/kubectl-commands#rolling-update)
å‘½ä»¤æ˜¯ä¸ªä¾‹å¤–ã€‚å¦‚æœæ‚¨æƒ³è¦æ»šåŠ¨æ›´æ–°åŠŸèƒ½è¯·è€ƒè™‘ä½¿ç”¨
Deploymentã€‚[`rolling-update`](/docs/reference/generated/kubectl/kubectl-commands#rolling-update)
å‘½ä»¤æ˜¯å¿…éœ€çš„ï¼Œè€Œ Deployment æ˜¯å£°æ˜æ€§çš„ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®é€šè¿‡
[`rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout)å‘½ä»¤ä½¿ç”¨
Deploymentã€‚

è™½ç„¶ ReplicaSets å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œä½†ä»Šå¤©å®ƒä¸»è¦
è¢«[Deployments](/docs/concepts/workloads/controllers/deployment/) ç”¨ä½œåè°ƒ Pod
åˆ›å»ºã€åˆ é™¤å’Œæ›´æ–°çš„æœºåˆ¶ã€‚å½“æ‚¨ä½¿ç”¨ Deployment æ—¶ï¼Œæ‚¨ä¸å¿…æ‹…å¿ƒè¿˜è¦ç®¡ç†å®ƒä»¬åˆ›å»ºçš„
ReplicaSetã€‚Deployment ä¼šæ‹¥æœ‰å¹¶ç®¡ç†å®ƒä»¬çš„ ReplicaSetã€‚

<!--
## When to use a ReplicaSet

A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.
-->

## ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ ReplicaSet

ReplicaSet ç¡®ä¿ä»»ä½•æ—¶é—´éƒ½æœ‰æŒ‡å®šæ•°é‡çš„ Pod å‰¯æœ¬åœ¨è¿è¡Œã€‚ç„¶è€Œï¼ŒDeployment æ˜¯ä¸€ä¸ªæ›´
é«˜çº§çš„æ¦‚å¿µï¼Œå®ƒç®¡ç† ReplicaSetï¼Œå¹¶å‘ Pod æä¾›å£°æ˜å¼çš„æ›´æ–°ä»¥åŠè®¸å¤šå…¶ä»–æœ‰ç”¨çš„åŠŸèƒ½ã€‚
å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ Deployment è€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ ReplicaSetï¼Œé™¤éæ‚¨éœ€è¦è‡ªå®šä¹‰æ›´æ–°ä¸š
åŠ¡æµç¨‹æˆ–æ ¹æœ¬ä¸éœ€è¦æ›´æ–°ã€‚

è¿™å®é™…ä¸Šæ„å‘³ç€ï¼Œæ‚¨å¯èƒ½æ°¸è¿œä¸éœ€è¦æ“ä½œ ReplicaSet å¯¹è±¡ï¼šè€Œæ˜¯ä½¿ç”¨ Deploymentï¼Œå¹¶åœ¨
spec éƒ¨åˆ†å®šä¹‰æ‚¨çš„åº”ç”¨ã€‚

<!--
## Example
-->

## ç¤ºä¾‹

{{< codenew file="controllers/frontend.yaml" >}}

<!--
Saving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster should
create the defined ReplicaSet and the pods that it manages.
-->

å°†æ­¤æ¸…å•ä¿å­˜åˆ° `frontend.yaml` ä¸­ï¼Œå¹¶å°†å…¶æäº¤åˆ° Kubernetes é›†ç¾¤ï¼Œåº”è¯¥å°±èƒ½åˆ›å»º
yaml æ–‡ä»¶æ‰€å®šä¹‰çš„ ReplicaSet åŠå…¶ç®¡ç†çš„ Podã€‚

```shell
$ kubectl create -f http://k8s.io/examples/controllers/frontend.yaml
replicaset.apps/frontend created
$ kubectl describe rs/frontend
Name:		frontend
Namespace:	default
Selector:	tier=frontend,tier in (frontend)
Labels:		app=guestbook
		tier=frontend
Annotations:	<none>
Replicas:	3 current / 3 desired
Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=guestbook
                tier=frontend
  Containers:
   php-redis:
    Image:      gcr.io/google_samples/gb-frontend:v3
    Port:       80/TCP
    Requests:
      cpu:      100m
      memory:   100Mi
    Environment:
      GET_HOSTS_FROM:   dns
    Mounts:             <none>
  Volumes:              <none>
Events:
  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----                -------------    --------    ------            -------
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-qhloh
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-9si5l
$ kubectl get pods
NAME             READY     STATUS    RESTARTS   AGE
frontend-9si5l   1/1       Running   0          1m
frontend-dnjpy   1/1       Running   0          1m
frontend-qhloh   1/1       Running   0          1m
```

<!--
## Writing a ReplicaSet Spec

As with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.  For
general information about working with manifests, see [object management using kubectl](/docs/concepts/overview/object-management-kubectl/overview/).

A ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status).
-->

## ç¼–å†™ ReplicaSet Spec

ä¸æ‰€æœ‰å…¶ä»– Kubernetes API å¯¹è±¡ä¸€æ ·ï¼ŒReplicaSet ä¹Ÿéœ€è¦ `apiVersion`ã€`kind`ã€å’Œ
`metadata` å­—æ®µã€‚æœ‰å…³ä½¿ç”¨æ¸…å•çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œè¯·å‚è§
[ä½¿ç”¨ kubectl ç®¡ç†å¯¹è±¡](/docs/concepts/overview/object-management-kubectl/overview/)ã€‚

ReplicaSet ä¹Ÿéœ€è¦
[`.spec`](https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status)
éƒ¨åˆ†ã€‚

<!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`. The `.spec.template` is a
[pod template](/docs/concepts/workloads/pods/pod-overview/#pod-templates). It has exactly the same schema as a
[pod](/docs/concepts/workloads/pods/pod/), except that it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields of a pod, a pod template in a ReplicaSet must specify appropriate
labels and an appropriate restart policy.

For labels, make sure to not overlap with other controllers. For more information, see [pod selector](#pod-selector).

For [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy), the only allowed value for `.spec.template.spec.restartPolicy` is `Always`, which is the default.

For local container restarts, ReplicaSet delegates to an agent on the node,
for example the [Kubelet](/docs/admin/kubelet/) or Docker.
-->

### Pod æ¨¡ç‰ˆ

`.spec.template` æ˜¯ `.spec` å”¯ä¸€éœ€è¦çš„å­—æ®µã€‚`.spec.template` æ˜¯
[Pod æ¨¡ç‰ˆ](/docs/concepts/workloads/pods/pod-overview/#pod-templates)ã€‚å®ƒå’Œ
[Pod](/docs/concepts/workloads/pods/pod/) çš„è¯­æ³•å‡ ä¹å®Œå…¨ä¸€æ ·ï¼Œé™¤äº†å®ƒæ˜¯åµŒå¥—çš„å¹¶æ²¡
æœ‰ `apiVersion` å’Œ `kind`ã€‚

é™¤äº†æ‰€éœ€çš„ Pod å­—æ®µä¹‹å¤–ï¼ŒReplicaSet ä¸­çš„ Pod æ¨¡æ¿å¿…é¡»æŒ‡å®šé€‚å½“çš„æ ‡ç­¾å’Œé€‚å½“çš„é‡å¯
ç­–ç•¥ã€‚

å¯¹äºæ ‡ç­¾ï¼Œè¯·ç¡®ä¿ä¸è¦ä¸å…¶ä»–æ§åˆ¶å™¨é‡å ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ
[Pod é€‰æ‹©å™¨](#pod-selector)ã€‚

å¯¹äº
[é‡å¯ç­–ç•¥](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)ï¼Œ`.spec.template.spec.restartPolicy`
å”¯ä¸€å…è®¸çš„å–å€¼æ˜¯ `Always`ï¼Œè¿™ä¹Ÿæ˜¯é»˜è®¤å€¼.

å¯¹äºæœ¬åœ°å®¹å™¨é‡æ–°å¯åŠ¨ï¼ŒReplicaSet å§”æ‰˜ç»™äº†èŠ‚ç‚¹ä¸Šçš„ä»£ç†å»æ‰§è¡Œï¼Œä¾‹
å¦‚[Kubelet](/docs/admin/kubelet/) æˆ– Docker å»æ‰§è¡Œã€‚

<!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). A ReplicaSet
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicaSet to be replaced without affecting the running pods.

The `.spec.template.metadata.labels` must match the `.spec.selector`, or it will
be rejected by the API.

In Kubernetes 1.9 the API version `apps/v1` on the ReplicaSet kind is the current version and is enabled by default. The API version `apps/v1beta2` is deprecated.
-->

### Pod é€‰æ‹©å™¨

`.spec.selector` å­—æ®µ
æ˜¯[æ ‡ç­¾é€‰æ‹©å™¨](/docs/concepts/overview/working-with-objects/labels/)ã€‚ReplicaSet
ç®¡ç†æ‰€æœ‰æ ‡ç­¾åŒ¹é…ä¸æ ‡ç­¾é€‰æ‹©å™¨çš„ Podã€‚å®ƒä¸åŒºåˆ†è‡ªå·±åˆ›å»ºæˆ–åˆ é™¤çš„ Pod å’Œå…¶ä»–äººæˆ–è¿›ç¨‹
åˆ›å»ºæˆ–åˆ é™¤çš„ podã€‚è¿™å…è®¸åœ¨ä¸å½±å“è¿è¡Œä¸­çš„ Pod çš„æƒ…å†µä¸‹æ›¿æ¢å‰¯æœ¬é›†ã€‚

`.spec.template.metadata.labels` å¿…é¡»åŒ¹é… `.spec.selector`ï¼Œå¦åˆ™å®ƒå°†è¢« API æ‹’ç»
ã€‚

Kubernetes 1.9 ç‰ˆæœ¬ä¸­ï¼ŒAPI ç‰ˆæœ¬ `apps/v1` ä¸­çš„ ReplicaSet ç±»å‹çš„ç‰ˆæœ¬æ˜¯å½“å‰ç‰ˆæœ¬å¹¶
é»˜è®¤å¼€å¯ã€‚API ç‰ˆæœ¬ `apps/v1beta2` è¢«å¼ƒç”¨ã€‚

<!--
Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicaSet, or with another controller such as a Deployment. If you do so, the ReplicaSet thinks that it
created the other pods. Kubernetes does not stop you from doing this.

If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself.
-->

å¦å¤–ï¼Œé€šå¸¸æ‚¨ä¸åº”è¯¥åˆ›å»ºæ ‡ç­¾ä¸æ­¤é€‰æ‹©å™¨åŒ¹é…çš„ä»»ä½• Podï¼Œæˆ–è€…ç›´æ¥ä¸å¦ä¸€ä¸ª ReplicaSet
æˆ–å¦ä¸€ä¸ªæ§åˆ¶å™¨ï¼ˆå¦‚ Deploymentï¼‰æ ‡ç­¾åŒ¹é…çš„ä»»ä½• Podã€‚å¦‚æœä½ è¿™æ ·åšï¼ŒReplicaSet ä¼šè®¤
ä¸ºå®ƒåˆ›é€ äº†å…¶ä»– Podã€‚Kubernetes å¹¶ä¸ä¼šé˜»æ­¢æ‚¨è¿™æ ·åšã€‚

å¦‚æœæ‚¨æœ€ç»ˆä½¿ç”¨äº†å¤šä¸ªå…·æœ‰é‡å é€‰æ‹©å™¨çš„æ§åˆ¶å™¨ï¼Œåˆ™å¿…é¡»è‡ªå·±è´Ÿè´£åˆ é™¤ã€‚

<!--
### Labels on a ReplicaSet

The ReplicaSet can itself have labels (`.metadata.labels`).  Typically, you
would set these the same as the `.spec.template.metadata.labels`.  However, they are allowed to be
different, and the `.metadata.labels` do not affect the behavior of the ReplicaSet.

### Replicas

You can specify how many pods should run concurrently by setting `.spec.replicas`. The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shut down, and a replacement starts early.

If you do not specify `.spec.replicas`, then it defaults to 1.
-->

### Replicas

é€šè¿‡è®¾ç½® `.spec.replicas` æ‚¨å¯ä»¥æŒ‡å®šè¦åŒæ—¶è¿è¡Œå¤šå°‘ä¸ª Podã€‚åœ¨ä»»ä½•æ—¶é—´è¿è¡Œçš„ Pod
æ•°é‡å¯èƒ½é«˜äºæˆ–ä½äº `.spec.replicas` æŒ‡å®šçš„æ•°é‡ï¼Œä¾‹å¦‚åœ¨å‰¯æœ¬åˆšåˆšè¢«å¢åŠ æˆ–å‡å°‘åã€æˆ–
è€… Pod æ­£åœ¨è¢«ä¼˜é›…åœ°å…³é—­ã€ä»¥åŠæ›¿æ¢æå‰å¼€å§‹ã€‚

å¦‚æœæ‚¨æ²¡æœ‰æŒ‡å®š `.spec.replicas`, é‚£ä¹ˆé»˜è®¤å€¼ä¸º 1ã€‚

<!--
## Working with ReplicaSets

### Deleting a ReplicaSet and its Pods

To delete a ReplicaSet and all of its Pods, use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The [Garbage collector](/docs/concepts/workloads/controllers/garbage-collection/) automatically deletes all of the dependent Pods by default.

When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Background` or `Foreground` in delete option. e.g. :
-->

## ä½¿ç”¨ ReplicaSets çš„å…·ä½“æ–¹æ³•

### åˆ é™¤ ReplicaSet å’Œå®ƒçš„ Pod

è¦åˆ é™¤ ReplicaSet å’Œå®ƒçš„æ‰€æœ‰ Podï¼Œä½¿
ç”¨[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)
å‘½ä»¤ã€‚é»˜è®¤æƒ…å†µä¸‹
ï¼Œ[åƒåœ¾æ”¶é›†å™¨](/docs/concepts/workloads/controllers/garbage-collection/) è‡ªåŠ¨åˆ
é™¤æ‰€æœ‰ä¾èµ–çš„ Podã€‚

å½“ä½¿ç”¨ REST API æˆ– `client-go` åº“æ—¶ï¼Œæ‚¨å¿…é¡»åœ¨åˆ é™¤é€‰é¡¹ä¸­å°† `propagationPolicy` è®¾
ç½®ä¸º `Background` æˆ– `Foreground`ã€‚ä¾‹å¦‚ï¼š

```shell
kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
> -H "Content-Type: application/json"
```

<!--
### Deleting just a ReplicaSet

You can delete a ReplicaSet without affecting any of its pods using [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) with the `--cascade=false` option.
When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`, e.g. :
-->

### åªåˆ é™¤ ReplicaSet

æ‚¨å¯ä»¥åªåˆ é™¤ ReplicaSet è€Œä¸å½±å“å®ƒçš„ Podï¼Œæ–¹æ³•æ˜¯ä½¿
ç”¨[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)
å‘½ä»¤å¹¶è®¾ç½® `--cascade=false` é€‰é¡¹ã€‚

å½“ä½¿ç”¨ REST API æˆ– `client-go` åº“æ—¶ï¼Œæ‚¨å¿…é¡»å°† `propagationPolicy` è®¾ç½®ä¸º
`Orphan`ã€‚ä¾‹å¦‚ï¼š

```shell
kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \
> -H "Content-Type: application/json"
```

<!--
Once the original is deleted, you can create a new ReplicaSet to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).
-->

ä¸€æ—¦åˆ é™¤äº†åŸæ¥çš„ ReplicaSetï¼Œå°±å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„æ¥æ›¿æ¢å®ƒã€‚ç”±äºæ–°æ—§ ReplicaSet çš„
`.spec.selector` æ˜¯ç›¸åŒçš„ï¼Œæ–°çš„ ReplicaSet å°†æ¥ç®¡è€çš„ Podã€‚ä½†æ˜¯ï¼Œå®ƒä¸ä¼šåŠªåŠ›ä½¿ç°
æœ‰çš„ Pod ä¸æ–°çš„ã€ä¸åŒçš„ Pod æ¨¡æ¿åŒ¹é…ã€‚è‹¥æƒ³è¦ä»¥å¯æ§çš„æ–¹å¼å°† Pod æ›´æ–°åˆ°æ–°çš„ specï¼Œ
å°±è¦ä½¿ç”¨ [æ»šåŠ¨æ›´æ–°](#rolling-updates)çš„æ–¹å¼ã€‚

<!--

### Isolating pods from a ReplicaSet

Pods may be removed from a ReplicaSet's target set by changing their labels. This technique may be used to remove pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
  assuming that the number of replicas is not also changed).

-->

### å°† Pod ä» ReplicaSet ä¸­éš”ç¦»

å¯ä»¥é€šè¿‡æ”¹å˜æ ‡ç­¾æ¥ä» ReplicaSet çš„ç›®æ ‡é›†ä¸­ç§»é™¤ Podã€‚è¿™ç§æŠ€æœ¯å¯ä»¥ç”¨æ¥ä»æœåŠ¡ä¸­å»é™¤
Podï¼Œä»¥ä¾¿è¿›è¡Œæ’é”™ã€æ•°æ®æ¢å¤ç­‰ã€‚ä»¥è¿™ç§æ–¹å¼ç§»é™¤çš„ Pod å°†è¢«è‡ªåŠ¨æ›¿æ¢ï¼ˆå‡è®¾å‰¯æœ¬çš„æ•°é‡
æ²¡æœ‰æ”¹å˜ï¼‰ã€‚

<!--
### Scaling a ReplicaSet

A ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller
ensures that a desired number of pods with a matching label selector are available and operational.
-->

### ç¼©æ”¾ RepliaSet

é€šè¿‡æ›´æ–° `.spec.replicas` å­—æ®µï¼ŒReplicaSet å¯ä»¥è¢«è½»æ¾çš„è¿›è¡Œç¼©æ”¾ã€‚ReplicaSet æ§åˆ¶
å™¨èƒ½ç¡®ä¿åŒ¹é…æ ‡ç­¾é€‰æ‹©å™¨çš„æ•°é‡çš„ Pod æ˜¯å¯ç”¨çš„å’Œå¯æ“ä½œçš„ã€‚

<!--
### ReplicaSet as an Horizontal Pod Autoscaler Target

A ReplicaSet can also be a target for
[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.
-->

### ReplicaSet ä½œä¸ºæ°´å¹³çš„ Pod è‡ªåŠ¨ç¼©æ”¾å™¨ç›®æ ‡

ReplicaSet ä¹Ÿå¯ä»¥ä½œä¸º
[æ°´å¹³çš„ Pod ç¼©æ”¾å™¨ (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/)
çš„ç›®æ ‡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒReplicaSet å¯ä»¥è¢« HPA è‡ªåŠ¨ç¼©æ”¾ã€‚ä»¥ä¸‹æ˜¯ HPA ä»¥æˆ‘ä»¬åœ¨å‰ä¸€ä¸ªç¤ºä¾‹
ä¸­åˆ›å»ºçš„å‰¯æœ¬é›†ä¸ºç›®æ ‡çš„ç¤ºä¾‹ã€‚

{{< codenew file="controllers/hpa-rs.yaml" >}}

<!--
Saving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated pods.
-->

å°†è¿™ä¸ªåˆ—è¡¨ä¿å­˜åˆ° `hpa-rs.yaml` å¹¶æäº¤åˆ° Kubernetes é›†ç¾¤ï¼Œå°±èƒ½åˆ›å»ºå®ƒæ‰€å®šä¹‰çš„
HPAï¼Œè¿›è€Œå°±èƒ½æ ¹æ®å¤åˆ¶çš„ Pod çš„ CPU åˆ©ç”¨ç‡å¯¹ç›®æ ‡ ReplicaSet è¿›è¡Œè‡ªåŠ¨ç¼©æ”¾ã€‚

```shell
kubectl create -f https://k8s.io/examples/controllers/hpa-rs.yaml
```

<!--
Alternatively, you can use the `kubectl autoscale` command to accomplish the same
(and it's easier!)
-->

æˆ–è€…ï¼Œå¯ä»¥ä½¿ç”¨ `kubectl autoscale` å‘½ä»¤å®Œæˆç›¸åŒçš„æ“ä½œã€‚ (è€Œä¸”å®ƒæ›´ç®€å•ï¼)

```shell
kubectl autoscale rs frontend
```

<!--
## Alternatives to ReplicaSet

### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying ReplicaSets and their Pods
in a similar fashion as `kubectl rolling-update`. Deployments are recommended if you want this rolling update functionality,
because unlike `kubectl rolling-update`, they are declarative, server-side, and have additional features. For more information on running a stateless
application using a Deployment, please read [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).
-->

## ReplicaSet çš„æ›¿ä»£æ–¹æ¡ˆ

### Deployment ï¼ˆæ¨èï¼‰

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) æ˜¯ä¸€ä¸ªé«˜çº§ API
å¯¹è±¡ï¼Œå®ƒä»¥ `kubectl rolling-update` çš„æ–¹å¼æ›´æ–°å…¶åº•å±‚å‰¯æœ¬é›†åŠå…¶ Podã€‚å¦‚æœæ‚¨éœ€è¦æ»š
åŠ¨æ›´æ–°åŠŸèƒ½ï¼Œå»ºè®®ä½¿ç”¨ Deploymentï¼Œå› ä¸º Deployment ä¸ `kubectl rolling-update` ä¸
åŒçš„æ˜¯ï¼šå®ƒæ˜¯å£°æ˜å¼çš„ã€æœåŠ¡å™¨ç«¯çš„ã€å¹¶ä¸”å…·æœ‰å…¶ä»–ç‰¹æ€§ã€‚æœ‰å…³ä½¿ç”¨ Deployment æ¥è¿è¡Œæ—
çŠ¶æ€åº”ç”¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
[ä½¿ç”¨ Deployment è¿è¡Œæ— çŠ¶æ€åº”ç”¨](/docs/tasks/run-application/run-stateless-application-deployment/)ã€‚

<!--
### Bare Pods

Unlike the case where a user directly created pods, a ReplicaSet replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
-->

### è£¸ Pod

ä¸ç”¨æˆ·ç›´æ¥åˆ›å»º Pod çš„æƒ…å†µä¸åŒï¼ŒReplicaSet ä¼šæ›¿æ¢é‚£äº›ç”±äºæŸäº›åŸå› è¢«åˆ é™¤æˆ–è¢«ç»ˆæ­¢çš„
Podï¼Œä¾‹å¦‚åœ¨èŠ‚ç‚¹æ•…éšœæˆ–ç ´åæ€§çš„èŠ‚ç‚¹ç»´æŠ¤ï¼ˆå¦‚å†…æ ¸å‡çº§ï¼‰çš„æƒ…å†µä¸‹ã€‚å› ä¸ºè¿™ä¸ªå¥½å¤„ï¼Œæˆ‘ä»¬
å»ºè®®æ‚¨ä½¿ç”¨ ReplicaSetï¼Œå³ä½¿åº”ç”¨ç¨‹åºåªéœ€è¦ä¸€ä¸ª Podã€‚æƒ³åƒä¸€ä¸‹ï¼ŒReplicaSet ç±»ä¼¼äºè¿›
ç¨‹ç›‘è§†å™¨ï¼Œåªä¸è¿‡å®ƒåœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šç›‘è§†å¤šä¸ª Podï¼Œè€Œä¸æ˜¯åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šç›‘è§†å•ä¸ªè¿›ç¨‹ã€‚
ReplicaSet å°†æœ¬åœ°å®¹å™¨é‡å¯çš„ä»»åŠ¡å§”æ‰˜ç»™äº†èŠ‚ç‚¹ä¸Šçš„æŸä¸ªä»£ç†ï¼ˆä¾‹å¦‚ï¼ŒKubelet æˆ–
Dockerï¼‰å»å®Œæˆã€‚

<!--
### Job

Use a [`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/) instead of a ReplicaSet for pods that are expected to terminate on their own
(that is, batch jobs).
-->

### Job

ä½¿ç”¨[`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/) ä»£æ›¿
ReplicaSetï¼Œå¯ä»¥ç”¨äºé‚£äº›æœŸæœ›è‡ªè¡Œç»ˆæ­¢çš„ Podã€‚

<!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
-->

### DaemonSet

å¯¹äºç®¡ç†é‚£äº›æä¾›ä¸»æœºçº§åˆ«åŠŸèƒ½ï¼ˆå¦‚ä¸»æœºç›‘æ§å’Œä¸»æœºæ—¥å¿—ï¼‰çš„å®¹å™¨ï¼Œå°±è¦
ç”¨[`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) è€Œä¸ç”¨
ReplicaSetã€‚è¿™äº› Pod çš„å¯¿å‘½ä¸ä¸»æœºå¯¿å‘½æœ‰å…³ï¼šè¿™äº› Pod éœ€è¦å…ˆäºä¸»æœºä¸Šçš„å…¶ä»– Pod è¿
è¡Œï¼Œå¹¶ä¸”åœ¨æœºå™¨å‡†å¤‡é‡æ–°å¯åŠ¨/å…³é—­æ—¶å®‰å…¨åœ°ç»ˆæ­¢ã€‚

{{% /capture %}}

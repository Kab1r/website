---
title: ä¸ºå®¹å™¨ç®¡ç†è®¡ç®—èµ„æº
content_template: templates/concept
weight: 20
feature:
  title: è‡ªåŠ¨è£…ç®±
  description: >
    æ ¹æ®èµ„æºéœ€æ±‚å’Œå…¶ä»–çº¦æŸè‡ªåŠ¨æ”¾ç½®å®¹å™¨ï¼ŒåŒæ—¶ä¸ä¼šç‰ºç‰²å¯ç”¨æ€§ï¼Œå°†ä»»åŠ¡å…³é”®å·¥ä½œè´Ÿè½½å’Œå°½åŠ›æœåŠ¡å·¥ä½œè´Ÿè½½è¿›è¡Œæ··åˆæ”¾ç½®ï¼Œä»¥æé«˜èµ„æºåˆ©ç”¨ç‡å¹¶èŠ‚çœæ›´å¤šèµ„æºã€‚
---

## <!--

title: Managing Compute Resources for Containers content_template:
templates/concept weight: 20 feature: title: Automatic binpacking description: >
Automatically places containers based on their resource requirements and other
constraints, while not sacrificing availability. Mix critical and best-effort
workloads in order to drive up utilization and save even more resources.

---

-->

{{% capture overview %}}

<!--
When you specify a [Pod](/docs/concepts/workloads/pods/pod/), you can optionally specify how
much CPU and memory (RAM) each Container needs. When Containers have resource
requests specified, the scheduler can make better decisions about which nodes to
place Pods on. And when Containers have their limits specified, contention for
resources on a node can be handled in a specified manner. For more details about
the difference between requests and limits, see
[Resource QoS](https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md).
-->

å½“æ‚¨å®šä¹‰ [Pod](/docs/user-guide/pods) çš„æ—¶å€™å¯ä»¥é€‰æ‹©ä¸ºæ¯ä¸ªå®¹å™¨æŒ‡å®šéœ€è¦çš„ CPU å’Œ
å†…å­˜ï¼ˆRAMï¼‰å¤§å°ã€‚å½“ä¸ºå®¹å™¨æŒ‡å®šäº†èµ„æºè¯·æ±‚åï¼Œè°ƒåº¦å™¨å°±èƒ½å¤Ÿæ›´å¥½çš„åˆ¤æ–­å‡ºå°†å®¹å™¨è°ƒåº¦åˆ°
å“ªä¸ªèŠ‚ç‚¹ä¸Šã€‚å¦‚æœæ‚¨è¿˜ä¸ºå®¹å™¨æŒ‡å®šäº†èµ„æºé™åˆ¶ï¼ŒKubernetes å°±å¯ä»¥æŒ‰ç…§æŒ‡å®šçš„æ–¹å¼æ¥å¤„ç†
èŠ‚ç‚¹ä¸Šçš„èµ„æºç«äº‰ã€‚å…³äºèµ„æºè¯·æ±‚å’Œé™åˆ¶çš„ä¸åŒç‚¹å’Œæ›´å¤šèµ„æ–™è¯·å‚è€ƒ
[Resource QoS](https://git.k8s.io/community/contributors/design-proposals/resource-qos.md)ã€‚

{{% /capture %}}

{{% capture body %}}

<!--
## Resource types
*CPU* and *memory* are each a *resource type*. A resource type has a base unit.
CPU is specified in units of cores, and memory is specified in units of bytes.

If you're using Kubernetes v1.14 or newer, you can specify _huge page_ resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.

For example, on a system where the default page size is 4KiB, you could specify a limit,
`hugepages-2Mi: 80Mi`. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.

{{< note >}}
You cannot overcommit `hugepages-*` resources.
This is different from the `memory` and `cpu` resources.
{{< /note >}}

CPU and memory are collectively referred to as *compute resources*, or just
*resources*. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
[API resources](/docs/concepts/overview/kubernetes-api/). API resources, such as Pods and
[Services](/docs/concepts/services-networking/service/) are objects that can be read and modified
through the Kubernetes API server.
-->

## èµ„æºç±»å‹

_CPU_ å’Œ*å†…å­˜*éƒ½æ˜¯*èµ„æºç±»å‹*ã€‚èµ„æºç±»å‹å…·æœ‰åŸºæœ¬å•ä½ã€‚CPU çš„å•ä½æ˜¯æ ¸å¿ƒæ•°ï¼Œå†…å­˜çš„å•
ä½æ˜¯å­—èŠ‚ã€‚

å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ Kubernetes v1.14 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œåˆ™å¯ä»¥æŒ‡å®šå·¨é¡µèµ„æºã€‚å·¨é¡µæ˜¯ Linux ç‰¹
æœ‰çš„åŠŸèƒ½ï¼ŒèŠ‚ç‚¹å†…æ ¸åœ¨å…¶ä¸­åˆ†é…çš„å†…å­˜å—æ¯”é»˜è®¤é¡µå¤§å°å¤§å¾—å¤šã€‚

ä¾‹å¦‚ï¼Œåœ¨é»˜è®¤é¡µé¢å¤§å°ä¸º 4KiB çš„ç³»ç»Ÿä¸Šï¼Œæ‚¨å¯ä»¥æŒ‡å®šä¸€ä¸ªé™åˆ¶
ï¼Œ`hugepages-2Mi: 80Mi`ã€‚å¦‚æœå®¹å™¨å°è¯•åˆ†é… 40 ä¸ª 2MiB å¤§é¡µé¢ï¼ˆæ€»å…± 80 MiB ï¼‰ï¼Œåˆ™
åˆ†é…å¤±è´¥ã€‚

{{< note >}} æ‚¨ä¸èƒ½è¿‡é‡ä½¿ç”¨`hugepages- *`èµ„æºã€‚è¿™ä¸`memory`å’Œ`cpu`èµ„æºä¸åŒã€‚
{{< /note >}}

CPU å’Œå†…å­˜ç»Ÿç§°ä¸º*è®¡ç®—èµ„æº*ï¼Œä¹Ÿå¯ä»¥ç§°ä¸º*èµ„æº*ã€‚è®¡ç®—èµ„æºçš„æ•°é‡æ˜¯å¯ä»¥è¢«è¯·æ±‚ã€åˆ†é…ã€
æ¶ˆè€—å’Œå¯æµ‹é‡çš„ã€‚å®ƒä»¬ä¸ [API èµ„æº](/docs/concepts/overview/kubernetes-api/) ä¸åŒ
ã€‚ API èµ„æºï¼ˆå¦‚ Pod å’Œ [Service](/docs/concepts/services-networking/service/)ï¼‰
æ˜¯å¯é€šè¿‡ Kubernetes API server è¯»å–å’Œä¿®æ”¹çš„å¯¹è±¡ã€‚

<!--
## Resource requests and limits of Pod and Container

Each Container of a Pod can specify one or more of the following:

* `spec.containers[].resources.limits.cpu`
* `spec.containers[].resources.limits.memory`
* `spec.containers[].resources.requests.cpu`
* `spec.containers[].resources.requests.memory`
Although requests and limits can only be specified on individual Containers, it
is convenient to talk about Pod resource requests and limits. A
*Pod resource request/limit* for a particular resource type is the sum of the
resource requests/limits of that type for each Container in the Pod.
-->

## Pod å’Œ å®¹å™¨çš„èµ„æºè¯·æ±‚å’Œé™åˆ¶

Pod ä¸­çš„æ¯ä¸ªå®¹å™¨éƒ½å¯ä»¥æŒ‡å®šä»¥ä¸‹çš„ä¸€ä¸ªæˆ–è€…å¤šä¸ªå€¼ï¼š

- `spec.containers[].resources.limits.cpu`
- `spec.containers[].resources.limits.memory`
- `spec.containers[].resources.requests.cpu`
- `spec.containers[].resources.requests.memory`

å°½ç®¡åªèƒ½åœ¨ä¸ªåˆ«å®¹å™¨ä¸ŠæŒ‡å®šè¯·æ±‚å’Œé™åˆ¶ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥æ–¹ä¾¿åœ°è®¡ç®—å‡º Pod èµ„æºè¯·æ±‚å’Œé™åˆ¶
ã€‚ç‰¹å®šèµ„æºç±»å‹çš„ Pod èµ„æºè¯·æ±‚/é™åˆ¶æ˜¯ Pod ä¸­æ¯ä¸ªå®¹å™¨çš„è¯¥ç±»å‹çš„èµ„æºè¯·æ±‚/é™åˆ¶çš„æ€»å’Œ
ã€‚

<!--
## Meaning of CPU

Limits and requests for CPU resources are measured in *cpu* units.
One cpu, in Kubernetes, is equivalent to:

- 1 AWS vCPU
- 1 GCP Core
- 1 Azure vCore
- 1 IBM vCPU
- 1 *Hyperthread* on a bare-metal Intel processor with Hyperthreading
Fractional requests are allowed. A Container with
`spec.containers[].resources.requests.cpu` of `0.5` is guaranteed half as much
CPU as one that asks for 1 CPU.  The expression `0.1` is equivalent to the
expression `100m`, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing. A
request with a decimal point, like `0.1`, is converted to `100m` by the API, and
precision finer than `1m` is not allowed. For this reason, the form `100m` might
be preferred.
CPU is always requested as an absolute quantity, never as a relative quantity;
0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.
-->

## CPU çš„å«ä¹‰

CPU èµ„æºçš„é™åˆ¶å’Œè¯·æ±‚ä»¥ _cpu_ ä¸ºå•ä½ã€‚

Kubernetes ä¸­çš„ä¸€ä¸ª cpu ç­‰äºï¼š

- 1 AWS vCPU
- 1 GCP Core
- 1 Azure vCore
- 1 _Hyperthread_ åœ¨å¸¦æœ‰è¶…çº¿ç¨‹çš„è£¸æœº Intel å¤„ç†å™¨ä¸Š

å…è®¸æµ®ç‚¹æ•°è¯·æ±‚ã€‚å…·æœ‰ `spec.containers[].resources.requests.cpu` ä¸º 0.5 çš„å®¹å™¨ä¿
è¯äº†ä¸€åŠ CPU è¦æ±‚ 1 CPU çš„ä¸€åŠã€‚è¡¨è¾¾å¼ `0.1` ç­‰ä»·äºè¡¨è¾¾å¼ `100m`ï¼Œå¯ä»¥çœ‹ä½œ â€œ100
millicpuâ€ã€‚æœ‰äº›äººè¯´æˆæ˜¯â€œä¸€ç™¾æ¯« cpuâ€ï¼Œå…¶å®è¯´çš„æ˜¯åŒæ ·çš„äº‹æƒ…ã€‚å…·æœ‰å°æ•°ç‚¹ï¼ˆå¦‚
`0.1`ï¼‰çš„è¯·æ±‚ç”± API è½¬æ¢ä¸º`100m`ï¼Œç²¾åº¦ä¸è¶…è¿‡ `1m`ã€‚å› æ­¤ï¼Œå¯èƒ½ä¼šä¼˜å…ˆé€‰æ‹© `100m`
çš„å½¢å¼ã€‚

CPU æ€»æ˜¯è¦ç”¨ç»å¯¹æ•°é‡ï¼Œä¸å¯ä»¥ä½¿ç”¨ç›¸å¯¹æ•°é‡ï¼›0.1 çš„ CPU åœ¨å•æ ¸ã€åŒæ ¸ã€48 æ ¸çš„æœºå™¨ä¸­
çš„æ„ä¹‰æ˜¯ä¸€æ ·çš„ã€‚

<!--
## Meaning of memory
Limits and requests for `memory` are measured in bytes. You can express memory as
a plain integer or as a fixed-point integer using one of these suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:
-->

## å†…å­˜çš„å«ä¹‰

å†…å­˜çš„é™åˆ¶å’Œè¯·æ±‚ä»¥å­—èŠ‚ä¸ºå•ä½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹åç¼€ä¹‹ä¸€ä½œä¸ºå¹³å‡æ•´æ•°æˆ–å®šç‚¹æ•´æ•°è¡¨ç¤ºå†…
å­˜ï¼šEï¼ŒPï¼ŒTï¼ŒGï¼ŒMï¼ŒKã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¸¤ä¸ªå­—æ¯çš„ç­‰æ•ˆçš„å¹‚æ•°ï¼šEiï¼ŒPiï¼ŒTi
ï¼ŒGiï¼ŒMiï¼ŒKiã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹ä»£è¡¨å¤§è‡´ç›¸åŒçš„å€¼ï¼š

```shell
128974848, 129e6, 129M, 123Mi
```

<!--
Here's an example.
The following Pod has two Containers. Each Container has a request of 0.25 cpu
and 64MiB (2<sup>26</sup> bytes) of memory. Each Container has a limit of 0.5
cpu and 128MiB of memory. You can say the Pod has a request of 0.5 cpu and 128
MiB of memory, and a limit of 1 cpu and 256MiB of memory.
-->

ä¸‹é¢æ˜¯ä¸ªä¾‹å­ã€‚

ä»¥ä¸‹ Pod æœ‰ä¸¤ä¸ªå®¹å™¨ã€‚æ¯ä¸ªå®¹å™¨çš„è¯·æ±‚ä¸º 0.25 cpu å’Œ 64MiBï¼ˆ2<sup>26</sup> å­—èŠ‚ï¼‰å†…
å­˜ï¼Œæ¯ä¸ªå®¹å™¨çš„é™åˆ¶ä¸º 0.5 cpu å’Œ 128MiB å†…å­˜ã€‚æ‚¨å¯ä»¥è¯´è¯¥ Pod è¯·æ±‚ 0.5 cpu å’Œ 128
MiB çš„å†…å­˜ï¼Œé™åˆ¶ä¸º 1 cpu å’Œ 256MiB çš„å†…å­˜ã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
    - name: db
      image: mysql
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "128Mi"
          cpu: "500m"
    - name: wp
      image: wordpress
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "128Mi"
          cpu: "500m"
```

<!--
## How Pods with resource requests are scheduled
When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
Containers is less than the capacity of the node. Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.
-->

## å…·æœ‰èµ„æºè¯·æ±‚çš„ Pod å¦‚ä½•è°ƒåº¦

å½“æ‚¨åˆ›å»ºä¸€ä¸ª Pod æ—¶ï¼ŒKubernetes è°ƒåº¦ç¨‹åºå°†ä¸º Pod é€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹ã€‚æ¯ä¸ªèŠ‚ç‚¹å…·æœ‰æ¯ç§
èµ„æºç±»å‹çš„æœ€å¤§å®¹é‡ï¼šå¯ä¸º Pod æä¾›çš„ CPU å’Œå†…å­˜é‡ã€‚è°ƒåº¦ç¨‹åºç¡®ä¿å¯¹äºæ¯ç§èµ„æºç±»å‹ï¼Œ
è°ƒåº¦çš„å®¹å™¨çš„èµ„æºè¯·æ±‚çš„æ€»å’Œå°äºèŠ‚ç‚¹çš„å®¹é‡ã€‚è¯·æ³¨æ„ï¼Œå°½ç®¡èŠ‚ç‚¹ä¸Šçš„å®é™…å†…å­˜æˆ– CPU èµ„
æºä½¿ç”¨é‡éå¸¸ä½ï¼Œä½†å¦‚æœå®¹é‡æ£€æŸ¥å¤±è´¥ï¼Œåˆ™è°ƒåº¦ç¨‹åºä»ç„¶æ‹’ç»åœ¨è¯¥èŠ‚ç‚¹ä¸Šæ”¾ç½® Podã€‚å½“èµ„æº
ä½¿ç”¨é‡ç¨åå¢åŠ æ—¶ï¼Œä¾‹å¦‚åœ¨è¯·æ±‚ç‡çš„æ¯æ—¥å³°å€¼æœŸé—´ï¼Œè¿™å¯ä»¥é˜²æ­¢èŠ‚ç‚¹ä¸Šçš„èµ„æºçŸ­ç¼ºã€‚

<!--
## How Pods with resource limits are run

When the kubelet starts a Container of a Pod, it passes the CPU and memory limits
to the container runtime.

When using Docker:
-->

## å…·æœ‰èµ„æºé™åˆ¶çš„ Pod å¦‚ä½•è¿è¡Œ

å½“ kubelet å¯åŠ¨ä¸€ä¸ª Pod çš„å®¹å™¨æ—¶ï¼Œå®ƒä¼šå°† CPU å’Œå†…å­˜é™åˆ¶ä¼ é€’åˆ°å®¹å™¨è¿è¡Œæ—¶ã€‚

å½“ä½¿ç”¨ Docker æ—¶ï¼š

<!--
- The `spec.containers[].resources.requests.cpu` is converted to its core value,
  which is potentially fractional, and multiplied by 1024. The greater of this number
  or 2 is used as the value of the
  [`--cpu-shares`](https://docs.docker.com/engine/reference/run/#cpu-share-constraint)
  flag in the `docker run` command.

- The `spec.containers[].resources.limits.cpu` is converted to its millicore value and
  multiplied by 100. The resulting value is the total amount of CPU time that a container can use
  every 100ms. A container cannot use more than its share of CPU time during this interval.
-->

- `spec.containers[].resources.requests.cpu` çš„å€¼å°†è½¬æ¢æˆ millicore å€¼ï¼Œè¿™æ˜¯ä¸ªæµ®
  ç‚¹æ•°ï¼Œå¹¶ä¹˜ä»¥ 1024ï¼Œè¿™ä¸ªæ•°å­—ä¸­çš„è¾ƒå¤§è€…æˆ– 2 ç”¨ä½œ `docker run` å‘½ä»¤ä¸­
  çš„[ `--cpu-shares`](https://docs.docker.com/engine/reference/run/#/cpu-share-constraint)
  æ ‡å¿—çš„å€¼ã€‚

- `spec.containers[].resources.limits.cpu` è¢«è½¬æ¢æˆ millicore å€¼ã€‚è¢«ä¹˜ä»¥ 100000
  ç„¶å é™¤ä»¥ 1000ã€‚è¿™ä¸ªæ•°å­—ç”¨ä½œ `docker run` å‘½ä»¤ä¸­çš„
  [`--cpu-quota`](https://docs.docker.com/engine/reference/run/#/cpu-quota-constraint)
  æ ‡å¿—çš„å€¼ã€‚[`--cpu-quota` ] æ ‡å¿—è¢«è®¾ç½®æˆäº† 100000ï¼Œè¡¨ç¤ºæµ‹é‡é…é¢ä½¿ç”¨çš„é»˜è®¤ 100ms
  å‘¨æœŸã€‚å¦‚æœ [`--cpu-cfs-quota`] æ ‡å¿—è®¾ç½®ä¸º trueï¼Œåˆ™ kubelet ä¼šå¼ºåˆ¶æ‰§è¡Œ cpu é™åˆ¶
  ã€‚ä» Kubernetes 1.2 ç‰ˆæœ¬èµ·ï¼Œæ­¤æ ‡å¿—é»˜è®¤ä¸º trueã€‚

<!--
  {{< note >}}
  The default quota period is 100ms. The minimum resolution of CPU quota is 1ms.
  {{</ note >}}
-->

{{< note >}} é»˜è®¤é…é¢é™åˆ¶ä¸º 100 æ¯«ç§’ã€‚ CPU é…é¢çš„æœ€å°å•ä½ä¸º 1 æ¯«ç§’ã€‚
{{</ note >}}

<!--
- The `spec.containers[].resources.limits.memory` is converted to an integer, and
  used as the value of the
  [`--memory`](https://docs.docker.com/engine/reference/run/#/user-memory-constraints)
  flag in the `docker run` command.
-->

- `spec.containers[].resources.limits.memory` è¢«è½¬æ¢ä¸ºæ•´å‹ï¼Œä½œä¸º `docker run` å‘½
  ä»¤ä¸­çš„
  [`--memory`](https://docs.docker.com/engine/reference/run/#/user-memory-constraints)
  æ ‡å¿—çš„å€¼ã€‚

<!--
If a Container exceeds its memory limit, it might be terminated. If it is
restartable, the kubelet will restart it, as with any other type of runtime
failure.

If a Container exceeds its memory request, it is likely that its Pod will
be evicted whenever the node runs out of memory.

A Container might or might not be allowed to exceed its CPU limit for extended
periods of time. However, it will not be killed for excessive CPU usage.

To determine whether a Container cannot be scheduled or is being killed due to
resource limits, see the
[Troubleshooting](#troubleshooting) section.
-->

å¦‚æœå®¹å™¨è¶…è¿‡å…¶å†…å­˜é™åˆ¶ï¼Œåˆ™å¯èƒ½ä¼šè¢«ç»ˆæ­¢ã€‚å¦‚æœå¯é‡æ–°å¯åŠ¨ï¼Œåˆ™ä¸æ‰€æœ‰å…¶ä»–ç±»å‹çš„è¿è¡Œæ—¶
æ•…éšœä¸€æ ·ï¼Œkubelet å°†é‡æ–°å¯åŠ¨å®ƒã€‚

å¦‚æœä¸€ä¸ªå®¹å™¨è¶…è¿‡å…¶å†…å­˜è¯·æ±‚ï¼Œé‚£ä¹ˆå½“èŠ‚ç‚¹å†…å­˜ä¸è¶³æ—¶ï¼Œå®ƒçš„ Pod å¯èƒ½è¢«é€å‡ºã€‚

å®¹å™¨å¯èƒ½è¢«å…è®¸ä¹Ÿå¯èƒ½ä¸è¢«å…è®¸è¶…è¿‡å…¶ CPU é™åˆ¶æ—¶é—´ã€‚ä½†æ˜¯ï¼Œç”±äº CPU ä½¿ç”¨ç‡è¿‡é«˜ï¼Œä¸ä¼š
è¢«æ€æ­»ã€‚

è¦ç¡®å®šå®¹å™¨æ˜¯å¦ç”±äºèµ„æºé™åˆ¶è€Œæ— æ³•å®‰æ’æˆ–è¢«æ€æ­»ï¼Œè¯·å‚é˜…[ç–‘éš¾è§£ç­”](#troubleshooting)
éƒ¨åˆ†ã€‚

<!--
## Monitoring compute resource usage

The resource usage of a Pod is reported as part of the Pod status.

If optional [tools for monitoring](/docs/tasks/debug-application-cluster/resource-usage-monitoring/)
are available in your cluster, then Pod resource usage can be retrieved either
from the [Metrics API](/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api)
directly or from your monitoring tools.
-->

## ç›‘æ§è®¡ç®—èµ„æºä½¿ç”¨

Pod çš„èµ„æºä½¿ç”¨æƒ…å†µè¢«æŠ¥å‘Šä¸º Pod çŠ¶æ€çš„ä¸€éƒ¨åˆ†ã€‚

å¦‚æœä¸ºé›†ç¾¤é…ç½®äº†å¯é€‰
[ç›‘æ§å·¥å…·](/docs/tasks/debug-application-cluster/resource-usage-monitoring/)ï¼Œåˆ™
å¯ä»¥ç›´æ¥ä»
[æŒ‡æ ‡ API](/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api)
æˆ–è€…ç›‘æ§å·¥å…·æ£€ç´¢ Pod èµ„æºçš„ä½¿ç”¨æƒ…å†µã€‚

<!--
## Troubleshooting

### My Pods are pending with event message failedScheduling

If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An event is produced each time the
scheduler fails to find a place for the Pod, like this:
-->

## ç–‘éš¾è§£ç­”

### æˆ‘çš„ Pod å¤„äº pending çŠ¶æ€ä¸”äº‹ä»¶ä¿¡æ¯æ˜¾ç¤º failedScheduling

å¦‚æœè°ƒåº¦å™¨æ‰¾ä¸åˆ°ä»»ä½•è¯¥ Pod å¯ä»¥åŒ¹é…çš„èŠ‚ç‚¹ï¼Œåˆ™è¯¥ Pod å°†ä¿æŒä¸å¯è°ƒåº¦çŠ¶æ€ï¼Œç›´åˆ°æ‰¾åˆ°
ä¸€ä¸ªå¯ä»¥è¢«è°ƒåº¦åˆ°çš„ä½ç½®ã€‚æ¯å½“è°ƒåº¦å™¨æ‰¾ä¸åˆ° Pod å¯ä»¥è°ƒåº¦çš„åœ°æ–¹æ—¶ï¼Œä¼šäº§ç”Ÿä¸€ä¸ªäº‹ä»¶ï¼Œ
å¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
kubectl describe pod frontend | grep -A 3 Events
```

```
Events:
  FirstSeen LastSeen   Count  From          Subobject   PathReason      Message
  36s   5s     6      {scheduler }              FailedScheduling  Failed for reason PodExceedsFreeCPU and possibly others
```

<!--
In the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on the node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:
- Add more nodes to the cluster.
- Terminate unneeded Pods to make room for pending Pods.
- Check that the Pod is not larger than all the nodes. For example, if all the
  nodes have a capacity of `cpu: 1`, then a Pod with a request of `cpu: 1.1` will
  never be scheduled.
You can check node capacities and amounts allocated with the
`kubectl describe nodes` command. For example:
-->

åœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œç”±äºèŠ‚ç‚¹ä¸Šçš„ CPU èµ„æºä¸è¶³ï¼Œåä¸º â€œfrontendâ€ çš„ Pod å°†æ— æ³•è°ƒåº¦ã€‚ç”±äº
å†…å­˜ä¸è¶³ï¼ˆPodExceedsFreeMemoryï¼‰ï¼Œç±»ä¼¼çš„é”™è¯¯æ¶ˆæ¯ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´å¤±è´¥ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœ
æœ‰è¿™ç§ç±»å‹çš„æ¶ˆæ¯è€Œå¤„äº pending çŠ¶æ€ï¼Œæ‚¨å¯ä»¥å°è¯•å¦‚ä¸‹å‡ ä»¶äº‹æƒ…ï¼š

- å‘é›†ç¾¤æ·»åŠ æ›´å¤šèŠ‚ç‚¹ã€‚
- ç»ˆæ­¢ä¸éœ€è¦çš„ Podï¼Œä¸ºå¾…å¤„ç†çš„ Pod è…¾å‡ºç©ºé—´ã€‚
- æ£€æŸ¥ Pod æ‰€éœ€çš„èµ„æºæ˜¯å¦å¤§äºæ‰€æœ‰èŠ‚ç‚¹çš„èµ„æºã€‚ ä¾‹å¦‚ï¼Œå¦‚æœå…¨éƒ¨èŠ‚ç‚¹çš„å®¹é‡
  ä¸º`cpuï¼š1`ï¼Œé‚£ä¹ˆä¸€ä¸ªè¯·æ±‚ä¸º `cpuï¼š1.1`çš„ Pod æ°¸è¿œä¸ä¼šè¢«è°ƒåº¦ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `kubectl describe nodes` å‘½ä»¤æ£€æŸ¥èŠ‚ç‚¹å®¹é‡å’Œåˆ†é…çš„æ•°é‡ã€‚ ä¾‹å¦‚ï¼š

```shell
kubectl describe nodes e2e-test-node-pool-4lw4
```

```
Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (12%)        1070Mi (14%)
```

<!--
In the preceding output, you can see that if a Pod requests more than 1120m
CPUs or 6.23Gi of memory, it will not fit on the node.

By looking at the `Pods` section, you can see which Pods are taking up space on
the node.

The amount of resources available to Pods is less than the node capacity, because
system daemons use a portion of the available resources. The `allocatable` field
[NodeStatus](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#nodestatus-v1-core)
gives the amount of resources that are available to Pods. For more information, see
[Node Allocatable Resources](https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md).
The [resource quota](/docs/concepts/policy/resource-quotas/) feature can be configured
to limit the total amount of resources that can be consumed. If used in conjunction
with namespaces, it can prevent one team from hogging all the resources.
-->

åœ¨ä¸Šé¢çš„è¾“å‡ºä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å¦‚æœ Pod è¯·æ±‚è¶…è¿‡ 1120m CPU æˆ–è€… 6.23Gi å†…å­˜ï¼ŒèŠ‚ç‚¹å°†æ—
æ³•æ»¡è¶³ã€‚

é€šè¿‡æŸ¥çœ‹ `Pods` éƒ¨åˆ†ï¼Œæ‚¨å°†çœ‹åˆ°å“ªäº› Pod å ç”¨çš„èŠ‚ç‚¹ä¸Šçš„èµ„æºã€‚

Pod å¯ç”¨çš„èµ„æºé‡å°äºèŠ‚ç‚¹å®¹é‡ï¼Œå› ä¸ºç³»ç»Ÿå®ˆæŠ¤ç¨‹åºä½¿ç”¨ä¸€éƒ¨åˆ†å¯ç”¨èµ„æºã€‚
[NodeStatus](/docs/resources-reference/{{< param "version" >}}/#nodestatus-v1-core)
çš„ `allocatable` å­—æ®µç»™å‡ºäº†å¯ç”¨äº Pod çš„èµ„æºé‡ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
[èŠ‚ç‚¹å¯åˆ†é…èµ„æº](https://git.k8s.io/community/contributors/design-proposals/node-allocatable.md)ã€‚

å¯ä»¥å°† [èµ„æºé…é¢](/docs/concepts/policy/resource-quotas/) åŠŸèƒ½é…ç½®ä¸ºé™åˆ¶å¯ä»¥ä½¿ç”¨
çš„èµ„æºæ€»é‡ã€‚å¦‚æœä¸ namespace é…åˆä¸€èµ·ä½¿ç”¨ï¼Œå°±å¯ä»¥é˜²æ­¢ä¸€ä¸ªå›¢é˜Ÿå ç”¨æ‰€æœ‰èµ„æºã€‚

<!--
### My Container is terminated
Your Container might get terminated because it is resource-starved. To check
whether a Container is being killed because it is hitting a resource limit, call
`kubectl describe pod` on the Pod of interest:
-->

## æˆ‘çš„å®¹å™¨è¢«ç»ˆæ­¢äº†

æ‚¨çš„å®¹å™¨å¯èƒ½å› ä¸ºèµ„æºæ¯ç«­è€Œè¢«ç»ˆæ­¢äº†ã€‚è¦æŸ¥çœ‹å®¹å™¨æ˜¯å¦å› ä¸ºé‡åˆ°èµ„æºé™åˆ¶è€Œè¢«æ€æ­»ï¼Œè¯·åœ¨
ç›¸å…³çš„ Pod ä¸Šè°ƒç”¨ `kubectl describe pod`ï¼š

```shell
kubectl describe pod simmemleak-hra99
```

```
Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Replication Controllers:        simmemleak (1/1 replicas created)
Containers:
  simmemleak:
    Image:  saadali/simmemleak
    Limits:
      cpu:                      100m
      memory:                   50Mi
    State:                      Running
      Started:                  Tue, 07 Jul 2015 12:54:41 -0700
    Last Termination State:     Terminated
      Exit Code:                1
      Started:                  Fri, 07 Jul 2015 12:54:30 -0700
      Finished:                 Fri, 07 Jul 2015 12:54:33 -0700
    Ready:                      False
    Restart Count:              5
Conditions:
  Type      Status
  Ready     False
Events:
  FirstSeen                         LastSeen                         Count  From                              SubobjectPath                       Reason      Message
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {scheduler }                                                          scheduled   Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   pulled      Pod container image "k8s.gcr.io/pause:0.8.0" already present on machine
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   created     Created with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   started     Started with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    spec.containers{simmemleak}         created     Created with docker id 87348f12526a
```

<!--
In the preceding example, the `Restart Count:  5` indicates that the `simmemleak`
Container in the Pod was terminated and restarted five times.

You can call `kubectl get pod` with the `-o go-template=...` option to fetch the status
of previously terminated Containers:
-->

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ`Restart Count: 5` æ„å‘³ç€ Pod ä¸­çš„ `simmemleak` å®¹å™¨è¢«ç»ˆæ­¢å¹¶é‡å¯
äº†äº”æ¬¡ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `kubectl get pod` å‘½ä»¤åŠ ä¸Š `-o go-template=...` é€‰é¡¹æ¥è·å–ä¹‹å‰ç»ˆæ­¢å®¹
å™¨çš„çŠ¶æ€ã€‚

```shell
kubectl get pod -o go-template='{{range.status.containerStatuses}}{{"Container Name: "}}{{.name}}{{"\r\nLastState: "}}{{.lastState}}{{end}}'  simmemleak-hra99
```

```
Container Name: simmemleak
LastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]]
```

<!--
You can see that the Container was terminated because of `reason:OOM Killed`, where `OOM` stands for Out Of Memory.
-->

æ‚¨å¯ä»¥çœ‹åˆ°å®¹å™¨å› ä¸º `reason:OOM killed` è¢«ç»ˆæ­¢ï¼Œ`OOM` è¡¨ç¤º Out Of Memoryã€‚

<!--
## Local ephemeral storage
{{< feature-state state="beta" >}}

Kubernetes version 1.8 introduces a new resource, _ephemeral-storage_ for managing local ephemeral storage. In each Kubernetes node, kubelet's root directory (/var/lib/kubelet by default) and log directory (/var/log) are stored on the root partition of the node. This partition is also shared and consumed by Pods via emptyDir volumes, container logs, image layers and container writable layers.

This partition is â€œephemeralâ€ and applications cannot expect any performance SLAs (Disk IOPS for example) from this partition. Local ephemeral storage management only applies for the root partition; the optional partition for image layer and writable layer is out of scope.

{{< note >}}
If an optional runtime partition is used, root partition will not hold any image layer or writable layers.
{{< /note >}}
-->

## æœ¬åœ°ä¸´æ—¶å­˜å‚¨

Kubernetes ç‰ˆæœ¬ 1.8 å¼•å…¥äº†æ–°èµ„æº*ephemeral-storage*ï¼Œç”¨äºç®¡ç†æœ¬åœ°ä¸´æ—¶å­˜å‚¨ã€‚åœ¨æ¯
ä¸ª Kubernetes èŠ‚ç‚¹ä¸­ï¼Œkubelet çš„æ ¹ç›®å½•ï¼ˆé»˜è®¤ä¸º /var/lib/kubeletï¼‰å’Œæ—¥å¿—ç›®å½•ï¼ˆ
/var/log ï¼‰å­˜å‚¨åœ¨èŠ‚ç‚¹çš„æ ¹åˆ†åŒºä¸Šã€‚ Pods è¿˜é€šè¿‡ emptyDir å·ï¼Œå®¹å™¨æ—¥å¿—ï¼Œé•œåƒå±‚å’Œå®¹
å™¨å¯å†™å±‚å…±äº«å’Œä½¿ç”¨æ­¤åˆ†åŒºã€‚

è¯¥åˆ†åŒºæ˜¯â€œä¸´æ—¶â€åˆ†åŒºï¼Œåº”ç”¨ç¨‹åºæ— æ³•ä»è¯¥åˆ†åŒºè·å¾—ä»»ä½•æ€§èƒ½ SLAï¼ˆä¾‹å¦‚ç£ç›˜ IOPSï¼‰ã€‚ æœ¬åœ°
ä¸´æ—¶å­˜å‚¨ç®¡ç†ä»…é€‚ç”¨äºæ ¹åˆ†åŒºã€‚ å›¾åƒå±‚å’Œå¯å†™å±‚çš„å¯é€‰åˆ†åŒºè¶…å‡ºèŒƒå›´ã€‚

{{< note >}} å¦‚æœä½¿ç”¨å¯é€‰çš„è¿è¡Œæ—¶åˆ†åŒºï¼Œåˆ™æ ¹åˆ†åŒºå°†ä¸ä¿å­˜ä»»ä½•é•œåƒå±‚æˆ–å¯å†™å±‚ã€‚
{{< /note >}}

<!--
### Requests and limits setting for local ephemeral storage
Each Container of a Pod can specify one or more of the following:
-->

### æœ¬åœ°ä¸´æ—¶å­˜å‚¨çš„è¯·æ±‚å’Œé™åˆ¶è®¾ç½®

Pod çš„æ¯ä¸ªå®¹å™¨å¯ä»¥æŒ‡å®šä»¥ä¸‹ä¸€é¡¹æˆ–å¤šé¡¹ï¼š

- `spec.containers[].resources.limits.ephemeral-storage`
- `spec.containers[].resources.requests.ephemeral-storage`

<!--
Limits and requests for `ephemeral-storage` are measured in bytes. You can express storage as
a plain integer or as a fixed-point integer using one of these suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:
-->

å¯¹â€œä¸´æ—¶å­˜å‚¨â€çš„é™åˆ¶å’Œè¯·æ±‚ä»¥å­—èŠ‚ä¸ºå•ä½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹åç¼€ä¹‹ä¸€å°†å­˜å‚¨è¡¨ç¤ºä¸ºçº¯æ•´æ•°æˆ–
å°æ•°å½¢å¼ï¼šEï¼ŒPï¼ŒTï¼ŒGï¼ŒMï¼ŒKã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ 2 çš„å¹‚æ¬¡æ–¹ï¼šEiï¼ŒPiï¼ŒTiï¼ŒGiï¼ŒMiï¼ŒKiã€‚ä¾‹
å¦‚ï¼Œä»¥ä¸‹å†…å®¹è¡¨ç¤ºçš„å€¼å…¶å®å¤§è‡´ç›¸åŒï¼š

```shell
128974848, 129e6, 129M, 123Mi
```

<!--
For example, the following Pod has two Containers. Each Container has a request of 2GiB of local ephemeral storage. Each Container has a limit of 4GiB of local ephemeral storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a limit of 8GiB of storage.
-->

ä¾‹å¦‚ï¼Œä»¥ä¸‹ Pod å…·æœ‰ä¸¤ä¸ªå®¹å™¨ã€‚æ¯ä¸ªå®¹å™¨éƒ½æœ‰ä¸€ä¸ª 2GiB çš„æœ¬åœ°ä¸´æ—¶å­˜å‚¨è¯·æ±‚ã€‚æ¯ä¸ªå®¹å™¨
çš„æœ¬åœ°ä¸´æ—¶å­˜å‚¨é™åˆ¶ä¸º 4GiBã€‚å› æ­¤ï¼Œè¯¥ Pod è¦æ±‚æœ¬åœ°ä¸´æ—¶å­˜å‚¨ç©ºé—´ä¸º 4GiBï¼Œå­˜å‚¨ç©ºé—´é™
åˆ¶ä¸º 8GiBã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
    - name: db
      image: mysql
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
      resources:
        requests:
          ephemeral-storage: "2Gi"
        limits:
          ephemeral-storage: "4Gi"
    - name: wp
      image: wordpress
      resources:
        requests:
          ephemeral-storage: "2Gi"
        limits:
          ephemeral-storage: "4Gi"
```

<!--
### How Pods with ephemeral-storage requests are scheduled

When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods. For more information, see ["Node Allocatable"](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable).

The scheduler ensures that the sum of the resource requests of the scheduled Containers is less than the capacity of the node.
-->

### å¦‚ä½•è°ƒåº¦ä¸´æ—¶å­˜å‚¨è¯·æ±‚çš„ Pod

åˆ›å»º Pod æ—¶ï¼ŒKubernetes è°ƒåº¦ç¨‹åºä¼šé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹æ¥è¿è¡Œ Podã€‚æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥ä¸º Pod
æä¾›æœ€å¤§æ•°é‡çš„æœ¬åœ°ä¸´æ—¶å­˜å‚¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚
è§[èŠ‚ç‚¹å¯åˆ†é…](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)ã€‚

è°ƒåº¦ç¨‹åºä¼šç¡®ä¿è°ƒåº¦çš„å®¹å™¨çš„èµ„æºè¯·æ±‚çš„æ€»å’Œå°äºèŠ‚ç‚¹çš„å®¹é‡ã€‚

<!--
### How Pods with ephemeral-storage limits run

For container-level isolation, if a Container's writable layer and logs usage exceeds its storage limit, the Pod will be evicted. For pod-level isolation, if the sum of the local ephemeral storage usage from all containers and also the Pod's emptyDir volumes exceeds the limit, the Pod will be evicted.
-->

### å…·æœ‰ä¸´æ—¶å­˜å‚¨é™åˆ¶çš„ Pod å¦‚ä½•è¿è¡Œ

å¯¹äºå®¹å™¨çº§éš”ç¦»ï¼Œå¦‚æœå®¹å™¨çš„å¯å†™å±‚å’Œæ—¥å¿—ä½¿ç”¨é‡è¶…å‡ºå…¶å­˜å‚¨é™åˆ¶ï¼Œåˆ™å°†é©±é€ Podã€‚å¯¹äº
pod çº§åˆ«çš„éš”ç¦»ï¼Œå¦‚æœæ¥è‡ªæ‰€æœ‰å®¹å™¨çš„æœ¬åœ°ä¸´æ—¶å­˜å‚¨ä½¿ç”¨é‡ä»¥åŠ Pod çš„ emptyDir å·çš„æ€»
å’Œè¶…è¿‡é™åˆ¶ï¼Œåˆ™å°†é©±é€ Podã€‚

<!--
### Monitoring ephemeral-storage consumption

When local ephemeral storage is used, it is monitored on an ongoing
basis by the kubelet.  The monitoring is performed by scanning each
emptyDir volume, log directories, and writable layers on a periodic
basis.  Starting with Kubernetes 1.15, emptyDir volumes (but not log
directories or writable layers) may, at the cluster operator's option,
be managed by use of [project
quotas](http://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html).
Project quotas were originally implemented in XFS, and have more
recently been ported to ext4fs.  Project quotas can be used for both
monitoring and enforcement; as of Kubernetes 1.15, they are available
as alpha functionality for monitoring only.
-->

### ç›‘æ§ä¸´æ—¶å­˜å‚¨æ¶ˆè€—

ä½¿ç”¨æœ¬åœ°ä¸´æ—¶å­˜å‚¨æ—¶ï¼Œkubelet ä¼šæŒç»­å¯¹æœ¬åœ°ä¸´æ—¶å­˜å‚¨æ—¶è¿›è¡Œç›‘è§†ã€‚é€šè¿‡å®šæœŸæ‰«æï¼Œæ¥ç›‘è§†
æ¯ä¸ª emptyDir å·ï¼Œæ—¥å¿—ç›®å½•å’Œå¯å†™å±‚ã€‚ä» Kubernetes 1.15 å¼€å§‹ï¼Œä½œä¸ºé›†ç¾¤æ“ä½œå‘˜çš„ä¸€
ä¸ªé€‰é¡¹ï¼Œå¯ä»¥é€š
è¿‡[é¡¹ç›®é…é¢](http://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html)
æ¥ç®¡ç† emptyDir å·ï¼ˆä½†æ˜¯ä¸åŒ…æ‹¬æ—¥å¿—ç›®å½•æˆ–å¯å†™å±‚ï¼‰ã€‚é¡¹ç›®é…é¢æœ€åˆæ˜¯åœ¨ XFS ä¸­å®ç°çš„
ï¼Œæœ€è¿‘åˆè¢«ç§»æ¤åˆ° ext4fs ä¸­ã€‚ é¡¹ç›®é…é¢å¯ç”¨äºç›‘è§†å’Œæ‰§è¡Œï¼› ä» Kubernetes 1.15 å¼€å§‹
ï¼Œå®ƒä»¬å¯ç”¨ä½œ Alpha åŠŸèƒ½ä»…ç”¨äºç›‘è§†ã€‚

<!--
Quotas are faster and more accurate than directory scanning.  When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.  If
a file is created and deleted, but with an open file descriptor, it
continues to consume space.  This space will be tracked by the quota,
but will not be seen by a directory scan.
-->

é…é¢æ¯”ç›®å½•æ‰«ææ›´å¿«ï¼Œæ›´å‡†ç¡®ã€‚å°†ç›®å½•åˆ†é…ç»™é¡¹ç›®æ—¶ï¼Œåœ¨è¯¥ç›®å½•ä¸‹åˆ›å»ºçš„æ‰€æœ‰æ–‡ä»¶éƒ½å°†åœ¨è¯¥
é¡¹ç›®ä¸­åˆ›å»ºï¼Œå†…æ ¸ä»…éœ€è·Ÿè¸ªè¯¥é¡¹ç›®ä¸­çš„æ–‡ä»¶æ­£åœ¨ä½¿ç”¨å¤šå°‘å—ã€‚å¦‚æœåˆ›å»ºå¹¶åˆ é™¤äº†æ–‡ä»¶ï¼Œä½†æ˜¯
æ–‡ä»¶æè¿°ç¬¦å·²æ‰“å¼€ï¼Œå®ƒå°†ç»§ç»­å ç”¨ç©ºé—´ã€‚ è¯¥ç©ºé—´å°†ç”±é…é¢è·Ÿè¸ªï¼Œä½†ç›®å½•æ‰«æä¸ä¼šæ£€æŸ¥ã€‚

<!--
Kubernetes uses project IDs starting from 1048576.  The IDs in use are
registered in `/etc/projects` and `/etc/projid`.  If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in `/etc/projects` and `/etc/projid` to prevent
Kubernetes from using them.
-->

Kubernetes ä½¿ç”¨ä» 1048576 å¼€å§‹çš„é¡¹ç›® IDã€‚æ­£åœ¨ä½¿ç”¨çš„ ID æ³¨å†Œäº `/etc/projects` å’Œ
`/etc/projid`ã€‚å¦‚æœæ­¤èŒƒå›´å†…çš„é¡¹ç›® ID ç”¨äºç³»ç»Ÿä¸Šçš„å…¶ä»–ç›®çš„ï¼Œåˆ™è¿™äº›é¡¹ç›® ID å¿…é¡»åœ¨
`/etc/projects` å’Œ `/etc/projid` ä¸­æ³¨å†Œï¼Œä»¥é˜²æ­¢ Kubernetes ä½¿ç”¨å®ƒä»¬ã€‚

<!--
To enable use of project quotas, the cluster operator must do the
following:

* Enable the `LocalStorageCapacityIsolationFSQuotaMonitoring=true`
  feature gate in the kubelet configuration.  This defaults to `false`
  in Kubernetes 1.15, so must be explicitly set to `true`.

* Ensure that the root partition (or optional runtime partition) is
  built with project quotas enabled.  All XFS filesystems support
  project quotas, but ext4 filesystems must be built specially.

* Ensure that the root partition (or optional runtime partition) is
  mounted with project quotas enabled.
-->

è¦å¯ç”¨é¡¹ç›®é…é¢ï¼Œé›†ç¾¤æ“ä½œå‘˜å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

- åœ¨ kubelet é…ç½®ä¸­å¯ç”¨ `LocalStorageCapacityIsolationFSQuotaMonitoring = true`
  åŠŸèƒ½ã€‚ åœ¨ Kubernetes 1.15 ä¸­é»˜è®¤ä¸º falseï¼Œå› æ­¤å¿…é¡»æ˜¾å¼è®¾ç½®ä¸º trueã€‚

- ç¡®ä¿æ ¹åˆ†åŒºï¼ˆæˆ–å¯é€‰çš„è¿è¡Œæ—¶åˆ†åŒºï¼‰æ˜¯åœ¨å¯ç”¨é¡¹ç›®é…é¢çš„æƒ…å†µä¸‹æ„å»ºçš„ã€‚ æ‰€æœ‰ XFS æ–‡ä»¶
  ç³»ç»Ÿéƒ½æ”¯æŒé¡¹ç›®é…é¢ï¼Œä½†æ˜¯ ext4 æ–‡ä»¶ç³»ç»Ÿå¿…é¡»ä¸“é—¨æ„å»ºã€‚

- ç¡®ä¿åœ¨å¯ç”¨äº†é¡¹ç›®é…é¢çš„æƒ…å†µä¸‹æŒ‚è½½äº†æ ¹åˆ†åŒºï¼ˆæˆ–å¯é€‰çš„è¿è¡Œæ—¶åˆ†åŒºï¼‰ã€‚

<!--
#### Building and mounting filesystems with project quotas enabled

XFS filesystems require no special action when building; they are
automatically built with project quotas enabled.

Ext4fs filesystems must be built with quotas enabled, then they must
be enabled in the filesystem:
-->

#### åœ¨å¯ç”¨é¡¹ç›®é…é¢çš„æƒ…å†µä¸‹æ„å»ºå’ŒæŒ‚è½½æ–‡ä»¶ç³»ç»Ÿ

XFS æ–‡ä»¶ç³»ç»Ÿåœ¨æ„å»ºæ—¶ä¸éœ€è¦ä»»ä½•ç‰¹æ®Šæ“ä½œ; å®ƒä»¬æ˜¯åœ¨å¯ç”¨é¡¹ç›®é…é¢çš„æƒ…å†µä¸‹è‡ªåŠ¨æ„å»ºçš„ã€‚

Ext4fs æ–‡ä»¶ç³»ç»Ÿå¿…é¡»åœ¨å¯ç”¨äº†é…é¢çš„æƒ…å†µä¸‹æ„å»ºï¼Œç„¶åå¿…é¡»åœ¨æ–‡ä»¶ç³»ç»Ÿä¸­å¯ç”¨å®ƒä»¬ï¼š

```
% sudo mkfs.ext4 other_ext4fs_args... -E quotatype=prjquota /dev/block_device
% sudo tune2fs -O project -Q prjquota /dev/block_device

```

<!--
To mount the filesystem, both ext4fs and XFS require the `prjquota`
option set in `/etc/fstab`:
-->

è¦æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿï¼Œext4fs å’Œ XFS éƒ½éœ€è¦åœ¨ `/etc/fstab` ä¸­è®¾ç½® `prjquota` é€‰é¡¹ï¼š

```
/dev/block_device	/var/kubernetes_data	defaults,prjquota	0	0
```

<!--
## Extended resources

Extended resources are fully-qualified resource names outside the
`kubernetes.io` domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.

There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.
-->

## æ‹“å±•èµ„æº

æ‹“å±•èµ„æºæ˜¯ `kubernetes.io` åŸŸåä¹‹å¤–çš„æ ‡å‡†èµ„æºåç§°ã€‚å®ƒä»¬å…è®¸é›†ç¾¤ç®¡ç†å‘˜åšåˆ†å‘ï¼Œè€Œ
ä¸”ç”¨æˆ·å¯ä»¥ä½¿ç”¨é Kubernetes å†…ç½®èµ„æºã€‚ä½¿ç”¨æ‰©å±•èµ„æºéœ€è¦ä¸¤ä¸ªæ­¥éª¤ã€‚ é¦–å…ˆï¼Œé›†ç¾¤ç®¡ç†
å‘˜å¿…é¡»åˆ†å‘æ‹“å±•èµ„æºã€‚ å…¶æ¬¡ï¼Œç”¨æˆ·å¿…é¡»åœ¨ Pod ä¸­è¯·æ±‚æ‹“å±•èµ„æºã€‚

```shell
curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
```

<!--
### Managing extended resources

#### Node-level extended resources

Node-level extended resources are tied to nodes.
-->

### ç®¡ç†æ‹“å±•èµ„æº

#### èŠ‚ç‚¹çº§æ‹“å±•èµ„æº

èŠ‚ç‚¹çº§æ‹“å±•èµ„æºç»‘å®šåˆ°èŠ‚ç‚¹ã€‚

<!--
##### Device plugin managed resources
See [Device
Plugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for how to advertise device plugin managed resources on each node.
-->

##### è®¾å¤‡æ’ä»¶æ‰˜ç®¡èµ„æº

æœ‰å…³å¦‚ä½•åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ†å‘è®¾å¤‡æ’ä»¶æ‰˜ç®¡èµ„æºçš„ä¿¡æ¯ï¼Œè¯·å‚
é˜…[è®¾å¤‡æ’ä»¶](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)ã€‚

<!--
##### Other resources
To advertise a new node-level extended resource, the cluster operator can
submit a `PATCH` HTTP request to the API server to specify the available
quantity in the `status.capacity` for a node in the cluster. After this
operation, the node's `status.capacity` will include a new resource. The
`status.allocatable` field is updated automatically with the new resource
asynchronously by the kubelet. Note that because the scheduler uses the	node
`status.allocatable` value when evaluating Pod fitness, there may be a short
delay between patching the node capacity with a new resource and the first Pod
that requests the resource to be scheduled on that node.
-->

##### å…¶ä»–èµ„æº

ä¸ºäº†å‘å¸ƒæ–°çš„èŠ‚ç‚¹çº§æ‹“å±•èµ„æºï¼Œé›†ç¾¤æ“ä½œå‘˜å¯ä»¥å‘ API æœåŠ¡å™¨æäº¤ `PATCH` HTTP è¯·æ±‚ï¼Œ
ä»¥åœ¨ `status.capacity` ä¸­ä¸ºé›†ç¾¤ä¸­çš„èŠ‚ç‚¹æŒ‡å®šå¯ç”¨æ•°é‡ã€‚å®Œæˆæ­¤æ“ä½œåï¼ŒèŠ‚ç‚¹çš„
`status.capacity` å°†åŒ…å«æ–°èµ„æºã€‚ç”± kubelet å¼‚æ­¥ä½¿ç”¨æ–°èµ„æºè‡ªåŠ¨æ›´æ–°
`status.allocatable` å­—æ®µã€‚è¯·æ³¨æ„ï¼Œç”±äºè°ƒåº¦ç¨‹åºåœ¨è¯„ä¼° Pod é€‚åˆæ€§æ—¶ä½¿ç”¨èŠ‚ç‚¹çš„çŠ¶æ€
`status.allocatable` å€¼ï¼Œå› æ­¤åœ¨ç”¨æ–°èµ„æºä¿®è¡¥èŠ‚ç‚¹å®¹é‡å’Œè¯·æ±‚åœ¨è¯¥èŠ‚ç‚¹ä¸Šè°ƒåº¦èµ„æºçš„ç¬¬
ä¸€ä¸ª Pod ä¹‹é—´å¯èƒ½ä¼šæœ‰çŸ­æš‚çš„å»¶è¿Ÿã€‚

<!--
**Example:**

Here is an example showing how to use `curl` to form an HTTP request that
advertises five "example.com/foo" resources on node `k8s-node-1` whose master
is `k8s-master`.
-->

**ç¤ºä¾‹:**

è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `curl` è¿›è¡Œ HTTP è¯·æ±‚ï¼Œè¯¥è¯·æ±‚åœ¨ä¸»èŠ‚ç‚¹ä¸º
`k8s-master` çš„å­èŠ‚ç‚¹ `k8s-node-1` ä¸Šé€šå‘Šäº”ä¸ª `example.com/foo` èµ„æºã€‚

```shell
curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
```

{{< note >}}

<!--
In the preceding request, `~1` is the encoding for the character `/`
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
-->

åœ¨å‰é¢çš„è¯·æ±‚ä¸­ï¼Œ`~1` æ˜¯ Patch è·¯å¾„ä¸­å­—ç¬¦ `/` çš„ç¼–ç ã€‚ JSON-Patch ä¸­çš„æ“ä½œè·¯å¾„å€¼
è¢«è§£é‡Šä¸º JSON-Pointerã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§
[IETF RFC 6901, section 3](https://tools.ietf.org/html/rfc6901#section-3).
{{< /note >}}

<!--
#### Cluster-level extended resources

Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.

You can specify the extended resources that are handled by scheduler extenders
in [scheduler policy
configuration](https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31).
-->

#### é›†ç¾¤çº§æ‰©å±•èµ„æº

ç¾¤é›†çº§æ‰©å±•èµ„æºä¸ç»‘å®šåˆ°èŠ‚ç‚¹ã€‚ å®ƒä»¬é€šå¸¸ç”±è°ƒåº¦ç¨‹åºæ‰©å±•ç¨‹åºç®¡ç†ï¼Œè¿™äº›ç¨‹åºå¤„ç†èµ„æºæ¶ˆ
è€—å’Œèµ„æºé…é¢ã€‚

æ‚¨å¯ä»¥
åœ¨[è°ƒåº¦ç¨‹åºç­–ç•¥é…ç½®](https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31)ä¸­
æŒ‡å®šç”±è°ƒåº¦ç¨‹åºæ‰©å±•ç¨‹åºå¤„ç†çš„æ‰©å±•èµ„æºã€‚

<!--
**Example:**

The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.

- The scheduler sends a Pod to the scheduler extender only if the Pod requests
     "example.com/foo".
- The `ignoredByScheduler` field specifies that the scheduler does not check
     the "example.com/foo" resource in its `PodFitsResources` predicate.
-->

**ç¤ºä¾‹:**

é€šè¿‡è°ƒåº¦ç¨‹åºç­–ç•¥çš„ä»¥ä¸‹é…ç½®ï¼ŒæŒ‡ç¤ºç¾¤é›†çº§æ‰©å±•èµ„æº "example.com/foo" ç”±è°ƒåº¦ç¨‹åºæ‰©å±•
ç¨‹åºå¤„ç†ã€‚

- ä»…å½“ Pod è¯·æ±‚ "example.com/foo" æ—¶ï¼Œè°ƒåº¦ç¨‹åºæ‰ä¼šå°† Pod å‘é€åˆ°è°ƒåº¦ç¨‹åºæ‰©å±•ç¨‹åº
  ã€‚
- `ignoredByScheduler` å­—æ®µæŒ‡å®šè°ƒåº¦ç¨‹åºä¸åœ¨å…¶ `PodFitsResources` å­—æ®µä¸­æ£€æŸ¥
  "example.com/foo" èµ„æºã€‚

```json
{
  "kind": "Policy",
  "apiVersion": "v1",
  "extenders": [
    {
      "urlPrefix": "<extender-endpoint>",
      "bindVerb": "bind",
      "managedResources": [
        {
          "name": "example.com/foo",
          "ignoredByScheduler": true
        }
      ]
    }
  ]
}
```

<!--
### Consuming extended resources

Users can consume extended resources in Pod specs just like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.

The API server restricts quantities of extended resources to whole numbers.
Examples of _valid_ quantities are `3`, `3000m` and `3Ki`. Examples of
_invalid_ quantities are `0.5` and `1500m`.
-->

### æ¶ˆè€—æ‰©å±•èµ„æº

å°±åƒ CPU å’Œå†…å­˜ä¸€æ ·ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨ Pod çš„æ‰©å±•èµ„æºã€‚è°ƒåº¦ç¨‹åºè´Ÿè´£æ ¸ç®—èµ„æºï¼Œå› æ­¤ä¸ä¼š
åŒæ—¶å°†è¿‡å¤šçš„å¯ç”¨èµ„æºåˆ†é…ç»™ Podã€‚

{{< note >}}

<!--
Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than `kubernetes.io` which is reserved.
-->

æ‰©å±•èµ„æºå–ä»£äº† Opaque æ•´æ•°èµ„æºã€‚ ç”¨æˆ·å¯ä»¥ä½¿ç”¨ä¿ç•™å­— `kubernetes.io` ä»¥å¤–çš„ä»»ä½•åŸŸ
åå‰ç¼€ã€‚

{{< /note >}}

<!--
To consume an extended resource in a Pod, include the resource name as a key
in the `spec.containers[].resources.limits` map in the container spec.
-->

è¦åœ¨ Pod ä¸­ä½¿ç”¨æ‰©å±•èµ„æºï¼Œè¯·åœ¨å®¹å™¨è§„èŒƒçš„ `spec.containers[].resources.limits` æ˜
å°„ä¸­åŒ…å«èµ„æºåç§°ä½œä¸ºé”®ã€‚

{{< note >}}

<!--
Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.
-->

æ‰©å±•èµ„æºä¸èƒ½è¿‡é‡ä½¿ç”¨ï¼Œå› æ­¤å¦‚æœå®¹å™¨è§„èŒƒä¸­å­˜åœ¨è¯·æ±‚å’Œé™åˆ¶ï¼Œåˆ™å®ƒä»¬å¿…é¡»ä¸€è‡´ã€‚

{{< /note >}}

<!--
A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the `PENDING` state
as long as the resource request cannot be satisfied.

**Example:**

The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).
-->

ä»…å½“æ»¡è¶³æ‰€æœ‰èµ„æºè¯·æ±‚(åŒ…æ‹¬ CPU ï¼Œå†…å­˜å’Œä»»ä½•æ‰©å±•èµ„æº)æ—¶ï¼Œæ‰èƒ½è°ƒåº¦ Podã€‚ åªè¦èµ„æºè¯·
æ±‚æ— æ³•æ»¡è¶³ï¼Œåˆ™ Pod ä¿æŒåœ¨ `PENDING` çŠ¶æ€ã€‚

**ç¤ºä¾‹:**

ä¸‹é¢çš„ Pod è¯·æ±‚ 2 ä¸ª CPU å’Œ 1 ä¸ª"example.com/foo"(æ‰©å±•èµ„æº)ã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: myimage
      resources:
        requests:
          cpu: 2
          example.com/foo: 1
        limits:
          example.com/foo: 1
```

{{% /capture %}}

{{% capture whatsnext %}}

<!--
* Get hands-on experience [assigning Memory resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/).

* Get hands-on experience [assigning CPU resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).

* [Container API](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#container-v1-core)

* [ResourceRequirements](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#resourcerequirements-v1-core)
-->

- è·å–å°†
  [åˆ†é…å†…å­˜èµ„æºç»™å®¹å™¨å’Œ Pod ](/docs/tasks/configure-pod-container/assign-memory-resource/)
  çš„å®è·µç»éªŒ

- è·å–å°†
  [åˆ†é… CPU èµ„æºç»™å®¹å™¨å’Œ Pod ](/docs/tasks/configure-pod-container/assign-cpu-resource/)
  çš„å®è·µç»éªŒ

- [å®¹å™¨](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#container-v1-core)

- [èµ„æºéœ€æ±‚](/docs/resources-reference/{{< param "version" >}}/#resourcerequirements-v1-core)

{{% /capture %}}
